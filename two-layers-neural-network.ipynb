{"cells":[{"metadata":{"_cell_guid":"b9e6cfa2-e7e4-49d3-9e4b-e64a175dfc16","_uuid":"0044260889076ec318a23206f393fc44f153489b"},"cell_type":"markdown","source":"#  Two Layers Neural Network:"},{"metadata":{"_cell_guid":"91bd8531-5821-43bf-a84b-cf1750266a85","_uuid":"c914bdf284e131d5e20c1119da2e445bc9e99766"},"cell_type":"markdown","source":"In the previous kernel, [Regression for XOR](https://www.kaggle.com/hamzafar/regression-for-xor) we have learned to build Regressoin for XOR fuction. To extend this concept we will consider whole operation in Regression as single unit. By whole operation we mean the multiplication of inputs with weights and addition of bias to them and applying non-linear function(sigmoid) to get activation values. The single unit, we say this *Neuron*, is shown below:"},{"metadata":{"_cell_guid":"a3cd9042-2e33-41f3-91a0-372e8406b45f","_uuid":"e5848158833bb1407241cf80a3046d1154735493"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Two%20Layers_Neural_Network/1.png?raw=true)"},{"metadata":{"_cell_guid":"7be5be0b-6ac8-4298-a2e6-9bedd877e4da","_uuid":"b76b91fac56016106b8cb965a67697f92023dcbb"},"cell_type":"markdown","source":"These neurons can be linked up together to from a network, commonly known as *Artificial Neural Network*. We say this artificial because this architecture is inspired from *Human Brain*.  The architecture mainly consists of *input layer, hidden layer, and output layer*. The *input layer and output layer* are conected to external world; and are connected to input information and target values respectively. In our case the input would be *Binary data of n_x length* and output would be *label generated from XOR operation from the same data.*\nThe term *hidden layer(s)* is a bit complex, as it may contain one or more hidden layers and with each layer there can be one or more *neuron*. The figure belw has 3-layer architecture, containing 2-hidden layer and 1-output layer. There are 4-hidden units(neuron) in each of the hidden layers. This architecture takes 3-input feature data."},{"metadata":{"_cell_guid":"ab99c658-5434-47b5-a298-693edd8a7d96","_uuid":"928552dd49f67ca9fc96151a486a2b118d3cee0e"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Two%20Layers_Neural_Network/2.jpeg?raw=true)\nIn this notebook we will do following:\n1. A function will generate data set of desired length and width (row, columns)\n1. The labelled is created using XOR; the deata genereted in the above step then passed to this step to get the value of XOR operation\n1. Implement two layer neural network model ( feed forward and back propagation)\n1. Discuss parameter dimensions  preservance."},{"metadata":{"_cell_guid":"8ab58017-a894-40df-8060-827a05dc44c7","_uuid":"c8cd2683068265550ae971a13a12068e7bc1707f","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # ploting graph","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"1bfd2b99-6191-46a1-8722-2ca3a80c8b88","_uuid":"4d72702b7cd31405676733c770d0ae9c1aec32a1"},"cell_type":"markdown","source":"### Generate Data\nTo discuss Generalized behavior of Regression model, by Generalized we mean it can work on any shape of data, we have created the generate_bits functions. The function is simple as it takes desired number of rows m and number of feature n_x and it randomly generated binary data i.e. 0 and 1."},{"metadata":{"_cell_guid":"3f092aba-6019-44ee-9d74-13bee5afb279","_uuid":"8389d86dcfea914bf041ee7e6b867edc41cd1c9f","collapsed":true,"trusted":true},"cell_type":"code","source":"def generate_bits(n_x, m):\n# Generate a m x n_x array of ints between 0 and 1, inclusive:\n# m: number of rows\n# n_x : number of columns per rows/ feature set\n    np.random.seed(1)\n    data = np.random.randint(2, size=(n_x, m))\n    return(data)\n","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"1c9b996f-f09f-497e-9a85-04a0d4e2966e","_uuid":"5601c36986b7c6f67c1209a5b3528f459b536fb9"},"cell_type":"markdown","source":"### Create Labels:\nFor training/updating derivatives of parameters weight and bias, the loss function determine the difference between actual values and the activation values. The actual value is the value that each example(row) has as it label. Like the the actual value of OR operation:\n\\begin{equation}\n 1+0=1=actualValue>:[oroperation=+]\\\\\n \\end{equation}\nThe generate_label function below takes data as input and apply XOR operation row wise."},{"metadata":{"_cell_guid":"0bd6b4f6-f8ac-4b7d-9b61-aa55f597e2e1","_uuid":"aad7b59bc8b89edbfbe4418250d4f47039108e6c","collapsed":true,"trusted":true},"cell_type":"code","source":"def generate_label(data, m):\n    # generate label by appyling xor operation to individual row\n    # return list of label (results)\n        # data: binary data set of m by n_x size\n    lst_y = []\n    y= np.empty((m,1))\n    k = 0\n    for tmp in data.T:\n        xor = np.logical_xor(tmp[0], tmp[1])\n\n        for i in range(2, tmp.shape[0]):\n            xor = np.logical_xor(xor, tmp[i])\n    #     print(xor)\n        lst_y.append(int(xor))\n        y[k,:] = int(xor)\n        k+=1\n    return(y.T)","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"30563b0c-89ca-4c86-b88e-06bcbc5c98b9","_uuid":"ce644ba5681696f533a740e1b86da87df70efbf4"},"cell_type":"markdown","source":"## Neural Network Architecture:\nWe will use *KEEP IT SO SIMPLE* rule and start with 2-layer neural network i.e. *1-hidden layer* and *1-output layer*. In the hidden layer we will use *tanh* as nonlinear function, and in the output layer *sigmoid* function is used.  Below is the figure of concept:\n\n![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Two%20Layers_Neural_Network/3.jpg?raw=true)"},{"metadata":{"_cell_guid":"f1372e89-1431-467c-a5e2-ca3c13efe6fc","_uuid":"cd0577d7ac4dc45a1197f3f6c3fe871cc3b8bfba"},"cell_type":"markdown","source":"in the above figure the paramater's subscript denoate the layer number i.e. $w_1$, $b_1$ show that these parameters are associated with *layer-1* while $w_2$, $b_2$ are for *layer-2*. the activations $a_1$, $a_2$  are related to *layer-1 and layer-2* and are output of *tanh and sigmoid* functions respectively. We can extend this concept to multiple hidden units in single neuron by stacking up multiple hidden units wiht non-linearity function for all hidden layers.\n\nThe following are the functions that will implement neural network architecture and update the parameters.\n\n** 1. layer_size:**\nThis function is responsible for creating accurate number of input uinits, hidden units and output units. The input units will be same as the feature set n_x of input data,  the hidden units will be according to user desired one and the output will be equal to target values, in our case we have one output layer as it can work for binary classification by giving *1* when *true* and *0* elsewehre."},{"metadata":{"_cell_guid":"9de1c493-4a32-44b3-ba83-28aeee697156","_uuid":"1535b74c3800f2df57940b7051009d44b54ac791","collapsed":true,"trusted":true},"cell_type":"code","source":"def layer_sizes(x, y, hidden):\n    # create neural network layers and return input, output and hidden units size\n        # x : imput data\n        # y : output labels\n        # hidden : number of hidden units in hidden layer\n    n_x = x.shape[0]\n    n_h = hidden\n    n_y = y.shape[0]\n    \n    return(n_x, n_h, n_y)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"70e5fcfa-53af-4eec-8711-696a2dbc957a","_uuid":"3c4ca925e9bff8bd4ac3026212ef42c257bacd1a"},"cell_type":"markdown","source":"** 2. initialize_param:**\nThis helper function will create parameter *w's* and *b's* for each of neural network layer; the inital value of *w's* will be random and *b's* will be zero. The size of *w's* and *b's*  will be as follow:\nSince, in the first hidden layer, the parameter *w's* are multiplied by *x's* i.e. each of input feature of *x* is multiplied by respective weight *w* and adding a bias *b* is resulted into single value *z* that is passed to non-linear function resulting a single unit (neuron). And if we have *n* number of neurons then the size of weight parameter will be of shape *(number of neurons, number of input feature)*   and the bias *b* will be of shape *(number of neurons, 1)*.\nWhen focus on the second layer of network, then the generic idea of parameter will be same but instead the shape changes according to layers presented there. So, we have paramter *w* of shape *(number of output values, number of neurons)* and bias *b* will be of shape *(number of output values,  1)*."},{"metadata":{"_cell_guid":"ad30b915-bb96-4a65-bbbb-7c48701066dc","_uuid":"b76d96a560aac926867f42b5bb655a790e1fe815","collapsed":true,"trusted":true},"cell_type":"code","source":"def initialize_param(n_x, n_h, n_y):\n    # intialize w to random and b to zero and return paramters\n        # n_x : number of input feature\n        # n_h : number of neurons\n        # n_y : number of output\n    np.random.seed(15)\n    \n    w1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros(shape=(n_h, 1))\n    w2 = np.random.rand(n_y, n_h) * 0.01\n    b2 = np.zeros(shape=(n_y, 1))\n    \n    parameters = {'w1' : w1,\n             'w2' : w2,\n             'b1' : b1,\n             'b2' : b2\n            }\n    return(parameters)","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"2d4fae90-f4b5-414c-8e43-6267fb3186b0","_uuid":"3f86dbb203730dee9ee731790cf3e6b4ff58da5b","collapsed":true,"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    # Takes input as z and return sogmoid of value\n    s = 1 / (1 + np.exp(-z))\n    return s","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"b903e950-9422-43e6-9d8f-b424045637a7","_uuid":"b4c9deaa9b96e84478715ef2c8b361b9cf6552b2"},"cell_type":"markdown","source":"** 3. initialize_param:**\nThis helper function compute the activation values from each of hidden neuron in each layer. Since we do have only two layers, therefore, in first layer it compute activations with *tanh* function in second layer it does with *sigmoid* function. $a_1, a_2$ referes to activations at layer one and two respectively."},{"metadata":{"_cell_guid":"143569e6-022f-4167-bd1b-7cc81527e850","_uuid":"59a98ebce80cc51bdc43b0eacfef2ca7c34e27d5","collapsed":true,"trusted":true},"cell_type":"code","source":"def forward_propagte(x,y, parameters):\n    # compute activations and return z and a\n        # x: input data\n        # y: target value\n        # parameters: dictonary object of w's and b's\n    z1 = np.dot(parameters['w1'], x) + parameters['b1']\n    a1 = np.tanh(z1)\n    z2 = np.dot(parameters['w2'], a1) + parameters['b2']\n    a2 = sigmoid(z2)\n    cache = {\"z1\": z1,\n         \"a1\": a1,\n         \"z2\": z2,\n         \"a2\": a2}\n    return(cache)","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"64ff7397-625f-43e7-bb9f-aac4a4b53aa7","_uuid":"8074960a9b3ed2bd119c9f844cd92584ab228be0"},"cell_type":"markdown","source":"If we consider input data *x* with $n_x = 2$ with training examples of 10, and having hidden neuron in first layer to *4* and output with single neuron; then the **Feed Forward** operation will be as of the following figure:\n![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Two%20Layers_Neural_Network/4.jpg?raw=true)\n"},{"metadata":{"_cell_guid":"dd8d663c-5bc7-4625-9735-c6f9bd0cf49a","_uuid":"79d64c0b14c3e1f47485ab375f3d9213dc3bf861"},"cell_type":"markdown","source":"**4. compute_cost: **\nThe compute cost function, calculate loss of each of the training example in the data set by comparing difference between actual and produce activation from the network and then average all the loss to get a single cost value."},{"metadata":{"_cell_guid":"8e809ac5-9d05-4031-ba25-a21b4987b460","_uuid":"45f7c7fa11c7a52c3010e7f8600f448c7477c5d0","collapsed":true,"trusted":true},"cell_type":"code","source":"def compute_cost(y, parameters, cache, m):\n    # calculate cost w.r.t. activation a2 and actual value y\n        # y: target value\n        # cache: dictionary of activations and z's\n        # m: number of training examples in data set\n    a = cache['a2']\n    loss = -1*(y* np.log(a) + (1-y) * np.log(1-a))\n    cost = np.sum(loss)/m\n    return(cost)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"ccfe0fdc-9ad5-42fe-8e0e-3e3b95c80e21","_uuid":"f77e922bddd99a57be876d0e6fa4d3cca6600e32"},"cell_type":"markdown","source":"### Derivative w.r.t cost:\n\nConsidering the [KISS](https://image.ibb.co/dGWSyS/Capture.jpg) graph above; we have derived partial derivatives of parameters *w's* and *b's*  for optimization purpose. $d$ subscript, in the starting of each variable represents the partial derivatives. One important thing to note here,  the $\\frac{\\partial{a_1}}{\\partial{z_1}}$ is derived as $1- a^2$. The detail understanding of $\\partial{tanh}$ can be found in the [blog post](http://ronny.rest/blog/post_2017_08_16_tanh/).\n\n\\begin{equation}\ndz_2  = a_2-y\\\\\ndb_2 = dz_2 ... (1)\\\\\ndw_2 = a_1 * dz_2 ... (2)\\\\\nda_1 = w_2 * dz_2\\\\\n\\begin{split}\ndz_1 & = \\frac{\\partial loss}{\\partial z_1}\\\\\n& = \\frac{\\partial loss}{\\partial a_1} * \\frac{\\partial a_1}{\\partial z_1}\\\\\nand:> \\frac{\\partial a_1}{\\partial z_1} & = tanh(a_1)\\\\\n& = 1- {a_1}^2\\\\\n\\end{split}\\\\\nso:> dz_1 = (w_2 * dz_2) * (1- {a_1}^2)\\\\\ndb_1 = dz_1 ... (3)\\\\\ndw_1 = x_1 * dz_1 ... (4)\\\\\n\\end{equation}\nAlthough the the graph only contains single variable for parameter at each layer but we can assign matrix to them to deal with big strucutre. "},{"metadata":{"_cell_guid":"ebdde167-8d7c-4703-922f-6c031cf4e1db","_uuid":"0ff71fe070a55e98d11a0eecd8aeeac586077265"},"cell_type":"markdown","source":"** 5. back_propagate: **\nIn the above derivations, we will use equation number $1, 2, 3$ and $4$ to compute partial derivatives of weights *w's* and bias *b's* in the back_propagate function. As of the compute cost, we will also average the values of derivatives by dividing it by *m*."},{"metadata":{"_cell_guid":"3fa06997-e635-407e-9ce6-b8fe904a36d8","_uuid":"c833fad66fc24f345274adf4128f43093140b9a2","collapsed":true,"trusted":true},"cell_type":"code","source":"def back_propagate(x, y, parameters, cache, m):\n    # compute and return derivatives of paramters \n        # x: input data\n        # y: target value\n        # parameters: dictonary object of w's and b's\n        # cache: dictionary of activations and z's\n        # m: number of training examples in data set\n        \n    a2 =cache['a2']\n    a1 =cache['a1']\n    w2 = parameters['w2']\n\n    dz2 = a2 - y\n\n    dw2 = (1 / m) * np.dot(dz2, a1.T)\n    db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)\n\n#     np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n    \n    dtan = 1 - np.power(a1, 2)\n    dz1 = np.dot(w2.T, dz2) * dtan\n\n    dw1 =(1 / m) * np.dot(dz1, x.T)\n    db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n    \n    grads = {'dw1': dw1,\n             'dw2': dw2,\n             'db1': db1,\n             'db2': db2,\n        }\n    return(grads)","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"a9f49b91-a79e-4a49-b5ce-98ee70febd0e","_uuid":"8e8abe879bf6463e165af4090f417c8c5bd2a195"},"cell_type":"markdown","source":"During **Back Propagation**, the consistency of the vectors/matrices of the paramters or states should be remain constant. Like the shape of any graph node during back propagation does not change due to maths operation. \nWe have showed, in each of the graph node, how the shape during *forward pass* and *backward pass* remain constant by attaching formula related to each node, and their resultinm matrix in the figure below and this will keep consist shape during multiple iteration in *Forward and Backward Pass*:"},{"metadata":{"_cell_guid":"a582dd4e-4372-4d1d-8d2c-8626c9183b1c","_uuid":"a031654e1beec6cf49f0919efbbd5b8beabbb984"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Two%20Layers_Neural_Network/5.png?raw=true)"},{"metadata":{"_cell_guid":"eddba82e-b48c-4bd8-830e-97322e9b6144","_uuid":"cc433f34426a3f0936ae04293bd31794f63466b1"},"cell_type":"markdown","source":"**6. update_parameters: **\nThis function update paramaters *w's* and *b's* according to the following equation:\n\\begin{equation}\nw := w - \\partial{w} * learningRate\\\\\nb := b - \\partial{b} * learningRate\n\\end{equation}"},{"metadata":{"_cell_guid":"0711679f-8b1c-4f7f-8ae4-c85122cb5ece","_uuid":"2a15e65bb22699914675800988b3774287b5390f","collapsed":true,"trusted":true},"cell_type":"code","source":"def update_parameters(parameters, grads, lr):\n    # update and return parameters \n        # parameters: dictonary object of w's and b's\n        # grads: dictionary object of gradient of w's and b's\n                \n    parameters['w1'] = parameters['w1'] -(lr * grads['dw1'])\n    parameters['w2'] = parameters['w2'] -(lr * grads['dw2'])\n\n    parameters['b1'] = parameters['b1'] -(lr * grads['db1'])\n    parameters['b2'] = parameters['b2'] -(lr * grads['db2'])\n\n    return(parameters)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"23d2cbaa-d9c4-4dc6-8093-ef427a3f6afb","_uuid":"4ab6e5afc7faa0e8f52411b28f0a652a6767db98","collapsed":true,"trusted":true},"cell_type":"code","source":"def optimize_parameters(x, y, parameters, m, num_iter):\n    # This function will iterate according to desired number, and update the paramters\n    # also return paramters and list of cost values.\n        # x: input data\n        # y: target value\n        # parameters: dictonary object of w's and b's\n        # m: number of training examples in data set\n        #num_iter: number of iteration to update parameters and comoute cost\n    lst_cost = []\n    \n    for i in range(num_iter):\n        cache = forward_propagte(x,y, parameters)\n    #     print('cost of ite:', compute_cost(y, parameters, cache, m))\n        grads = back_propagate(x, y, parameters, cache, m)\n        parameters = update_parameters(parameters, grads, lr)\n        lst_cost.append(compute_cost(y, parameters, cache, m))\n    return(parameters, lst_cost)","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"52d69e19-a3ca-4128-a466-bac0d20c3506","_uuid":"23eb0845e7709ecae8b764a3e7bf7426182007e4","collapsed":true,"trusted":true},"cell_type":"code","source":"def plt_res(lst, ylab, lr):\n    #This will plot the list of values at y axis while x axis will contain number of iteration\n    #lst: lst of action/cost\n    #ylab: y-axis label\n    #lr: learning rate\n    plt.plot(lst)\n    plt.ylabel(ylab)\n    plt.xlabel('iterations')\n    plt.title(\"Learning rate =\" + str(lr))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c09e8425-9059-416b-9aff-71f4b2569ec4","_uuid":"a657feb8159d890b1a094d0cf5edd99451f6e50e"},"cell_type":"markdown","source":"Now, we  generate data set with *10000*  and *100000* training examples with feature set equal to *50*. The paramaters *w's* and *b's* are updated about *1000* times.  The neural network architecture contains *10 hidden neuron* in first layers. The learining rate is set to 0.07. At the end the *cost* is plotted on *y-axis* against *number of iteration* on x-axis to see the behavior of reduction in cost."},{"metadata":{"_cell_guid":"e35e29fc-4a71-4f8b-b2f4-da20d66912e0","_uuid":"d0d27ffd476f95803311cefcaee29aad8c221687","trusted":true},"cell_type":"code","source":"n_x = 50\nm = 10000\nlr = 0.07\nnum_iter = 1000\n\nx = generate_bits(n_x, m)\ny = generate_label(x, m)\n\nn_x, n_h, n_y = layer_sizes(x, y, 10)\nparameters = initialize_param(n_x, n_h, n_y)\n\nparameters, lst_cost_s = optimize_parameters(x, y, parameters, m, num_iter)\n\n################################################################################\nm = 100000\n\nx = generate_bits(n_x, m)\ny = generate_label(x, m)\n\nn_x, n_h, n_y = layer_sizes(x, y, 10)\nparameters = initialize_param(n_x, n_h, n_y)\n\nparameters, lst_cost_m = optimize_parameters(x, y, parameters, m, num_iter)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"698c60eb-066b-4850-bd9d-700f595ec34c","_uuid":"15d90ed15b1e576c9c3328e1d5eaf1a272094212","trusted":true},"cell_type":"code","source":"print('------- 10000 training sample-------------')\nplt_res(lst_cost_s, 'cost', lr)\nprint('------- 100000 training sample-------------')\nplt_res(lst_cost_m, 'cost', lr)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"148d79da-313e-43dc-97ba-eca738f3261f","_uuid":"234d64e905ae5cc44b775b85a66382a5790ae5b3","collapsed":true},"cell_type":"markdown","source":"----"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}