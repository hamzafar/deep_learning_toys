{"cells":[{"metadata":{"_cell_guid":"8cebdecc-1f79-4b52-8d57-255bc4168e02","_uuid":"67698428d43cefca22beee8d1a74f3c06169127b"},"cell_type":"markdown","source":"# Recurrent Neural Network:\n\nThe Simple Neural Network takes features as input $x$ and optimize error. While Recurrent Neural Network not only learns from features but also take cares of sequence values over the time. It stores the information at each state and passes it to other state of network so mimic the 'memory'.\n\nIn this notebook we will do following:\n1. Description of Recurrent Recurrnet Network\n2. Deployment of Recurrent Network\n3. Performance of Network on different length of training examples"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # ploting graph","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"### Generate Data\nTo discuss Generalized behavior of Regression model, by Generalized we mean it can work on any shape of data, we have created the generate_bits functions. The function is simple as it takes desired number of rows m and number of feature n_x and it randomly generated binary data i.e. 0 and 1."},{"metadata":{"_cell_guid":"6c359896-e030-4267-8ef4-ea5a2bb88b1a","_uuid":"230e8cb036d25acb48a7b151bb3fd8d147f06818","collapsed":true,"trusted":true},"cell_type":"code","source":"def generate_bits(n_x, m):\n# Generate a m x n_x array of ints between 0 and 1, inclusive:\n# m: number of rows\n# n_x : number of columns per rows/ feature set\n    np.random.seed(1)\n    data = np.random.randint(2, size=(n_x, m))\n    return(data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6bd844fe-28bc-4957-b1d4-7a9e86fb08a5","_uuid":"ffded897491962dbb34f83ff52affe47848dcce6"},"cell_type":"markdown","source":"Create Labels:\nFor training/updating derivatives of parameters weight and bias, the loss function determine the difference between actual values and the activation values. The actual value is the value that each example(row) has as it label. Like the the actual value of OR operation:\n$$1+0=1=actualValue>:[oroperation=+]$$\nThe generate_label function below takes data as input and apply XOR operation row wise."},{"metadata":{"_cell_guid":"53a190a8-8a02-4adc-a3ee-fd82bb72bcd2","_uuid":"8fc8548cca32af2b1331304f28489180b6dbaaf1","collapsed":true,"trusted":true},"cell_type":"code","source":"def generate_label(data, m):\n    # generate label by appyling xor operation to individual row\n    # return list of label (results)\n        # data: binary data set of m by n_x size\n    lst_y = []\n    y= np.empty((m,1))\n    k = 0\n    for tmp in data.T:\n        xor = np.logical_xor(tmp[0], tmp[1])\n\n        for i in range(2, tmp.shape[0]):\n            xor = np.logical_xor(xor, tmp[i])\n    #     print(xor)\n        lst_y.append(int(xor))\n        y[k,:] = int(xor)\n        k+=1\n    return(y.T)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c232b9c5-7980-4c50-a7ad-d6eed0e1f8fe","_uuid":"2ede79895a61324e6e1c8df61361ca4670d104b4"},"cell_type":"markdown","source":"## Forward Propagation:\n\nThe Recurrenet Neural Network, is network that loop over it, passing information to itself timely. We may say that at input to the network we have input features and activations on the input is computed in the RNN-Cell whose activation is used for computing next step activation based on next input features and for the prediction purpose. This iteration goes up the length of the input sequence.  Let's see the following figure to get insipration of architecture:\n"},{"metadata":{"_cell_guid":"c813d832-6431-401b-8aaf-2281bbdcd593","_uuid":"71ea4c5e3956cdcae80aa97e302027c1e8336210"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/rnn/3.JPG?raw=true)"},{"metadata":{"_cell_guid":"8c4ab07d-cb82-4c0e-a181-5cd8ab882c01","_uuid":"617b06b5072f80a804857cd85b26ce6c764d2021"},"cell_type":"markdown","source":"The above is generic architecture, which takes input feature $x$ at time step $t$ and whose activation $a$ and output $y^{t}$ is computed on the $x$ and previous time step activation$a^{t-1}$. Unrolling the above network resulted in following architecture. Where the each input and output values to RNN-cell is marked with time step $0$ to $t$.\n"},{"metadata":{"_cell_guid":"38f0e789-a796-461e-abbb-f3cb3d696dee","_uuid":"94e4f2defc61f340faaa950badce7fccc6ecc230"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/rnn/4.JPG?raw=true)"},{"metadata":{"_cell_guid":"701e54cb-eb15-4f42-b708-8a44cf29afee","_uuid":"da30b5285ea0e5f35e5a0cb43379b1bd95511a09"},"cell_type":"markdown","source":"The operations in RNN-Cell consists of computing linear activations of input feature $x$ by multiplying it with some paramters $W_{ax}$ and adding some bias $b_a$. The previous activation information is also added to this step as $a^{t-1}$ is mulyiplied by parameter $W_{aa}$ and this is also added to the results of values we have achieved by input value $x$, parameter $W_{ax}$ and bias $b_a$.\nThe computed value is then passed from some non-linear function, we use $tanh$ that resulted in activations $a$. The same activation is used to compute prediction by using some function $sigmoid$ or $softmax$.\nThe following diagram shows the process for sinle RNN-cell:"},{"metadata":{"_cell_guid":"f262d831-61fc-4350-bb46-c0d1c5bd7ba1","_uuid":"6cda6cd49371dea9d88003e6fbeb093fe56d5f21"},"cell_type":"markdown","source":"![](https://image.ibb.co/fdXpbx/Capture.jpg)"},{"metadata":{"_cell_guid":"8aed98c6-8479-46ef-998b-1a3c951ae033","_uuid":"3ae05613ea0fd4b39c16637abcdcd94429dd4d2c"},"cell_type":"markdown","source":"So, the equations to compute the RNN-Cell will be as follow:\n$$a^{(t)} = tanh(W_{ax}*x^{(t)} + W_{aa}*a^{(t-1)} + b_a)$$\n$$\\hat{y}^{(t)} = sigmoid(W_{ya}*a^{(t)} + b_y)$$"},{"metadata":{"_cell_guid":"e7f0464b-b080-4a79-8741-fb8ba7c242f6","_uuid":"b214dde7d16a7029f29777bc63e7a1ecd1bfbb68"},"cell_type":"markdown","source":"**sigmoid:** We are using sigmoid function to predcit $\\hat{y}$ as we only have two posible outcomes as $0$ or $1$ as target variable."},{"metadata":{"_cell_guid":"16470a40-972a-4e83-8e83-4bbc0e8fadaa","_uuid":"fd9b43cd259697c2956ae662ecb7ecf6300c02be","collapsed":true,"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    # determine sigmoid of value\n        #z: linear activation\n    s = 1 / (1 + np.exp(-z))\n    return s","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d29861b1-2350-4719-9c40-0f2bfc61abac","_uuid":"1da752cd315db6c727788d6e4b647d991b7cced0"},"cell_type":"markdown","source":"**rnn_cell_compute:** This helper function is responsible to determine activations and predicted values for single RNN-Cell. The Function takes input feature $x$ at single time stemp, parameters and previous activations."},{"metadata":{"_cell_guid":"5504fcb9-4483-4d87-8580-530c7006b2fa","_uuid":"b688ade7cf895c6f74d8823b484c878e4a6771d4","collapsed":true,"trusted":true},"cell_type":"code","source":"def rnn_cell_compute(xt, a_prev, parameters):\n    # Computes and returns activations, predcitions yhat\n        #xt: input to current state\n        #a_prev: activation from previous state\n        #parameters: dictonary object of weights and bias\n        \n    Wax = parameters[\"Wax\"]\n    Waa = parameters[\"Waa\"]\n    Wya = parameters[\"Wya\"]\n    ba = parameters[\"ba\"]\n    by = parameters[\"by\"]\n    \n    # compute next activation state using the formula given above\n    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n    # compute output of the current cell using the formula given above\n    yt_pred = sigmoid(np.dot(Wya, a_next) + by)\n\n    # store values you need for backward propagation in cache\n    cache = (a_next, a_prev, xt, parameters)\n    \n    return a_next, yt_pred, cache","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58602d49-482c-4df1-b866-48f3d1fab07c","_uuid":"a9304d94405bc3a811b46f80516addda1cec4f3f"},"cell_type":"markdown","source":"So, the RNN unfolded architecture will be combining each single RNN-Cell and get input of features $x$ and activations from the previovs sates $a^{(t-1)}$. We have combined each RNN-Cell to demonstrate the architecture over $t$ time sequence.\n![](https://image.ibb.co/bWAjbx/Untitled.png)"},{"metadata":{"_cell_guid":"7f8a692e-9b8b-49d8-9fa2-72ab0b5bc263","_uuid":"450238a1b3ed1625dfec861b0058e3ed3e56e993"},"cell_type":"markdown","source":"**rnn_forward:** the generic function, that computes the activations and predictions based on the length of sequence of data. This function iterates about the length of sequence $t$ and get activations and predictions from the **rnn_cell_compute**. The function takes $x$ input data, previous activations and parameters."},{"metadata":{"_cell_guid":"5a1f2473-88e5-4c04-ab21-2997eed6df57","_uuid":"644de975247252d1bb0711d86bd998c748523fd7","collapsed":true,"trusted":true},"cell_type":"code","source":"def rnn_forward(x, a0, parameters):\n    # Itereate over the rnn_cell_compute function to compute activations, predictions for the states length equal to time stamp\n        #x: input data with feature set and time stemp t_x\n        #a0: activations at previous state\n        #parameters: dictonary object of weights and bias\n\n    caches = []\n    \n    # Retrieve dimensions from shapes of x and Wy\n    n_x, m, T_x = x.shape\n    n_y, n_a = parameters[\"Wya\"].shape\n    \n    ### START CODE HERE ###\n    \n    # initialize \"a\" and \"y\" with zeros \n    a = np.zeros((n_a, m, T_x))\n    y_pred = np.zeros((n_y, m, T_x))\n    \n    # Initialize a_next \n    a_next = a0\n    \n    # loop over all time-steps\n    for t in range(T_x):\n        # Update next hidden state, compute the prediction, get the cache \n        a_next, yt_pred, cache = rnn_cell_compute(x[:,:,t], a_next, parameters)\n        # Save the value of the new \"next\" hidden state in a (≈1 line)\n        a[:,:,t] = a_next\n        # Save the value of the prediction in y \n        y_pred[:,:,t] = yt_pred\n        # Append \"cache\" to \"caches\" \n        caches.append(cache)\n            \n    # store values needed for backward propagation in cache\n    caches = (caches, x)\n    \n    return a, y_pred, caches","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1961674f-63fe-4c54-91a1-15d716589fa7","_uuid":"a4b0056cf0abeac71892706b93ce2cc895f6d572"},"cell_type":"markdown","source":"**initialize_param:**  This function generates random arrays for the paramaters used in Recuurent Netowrk.  It takes arguments like number of activation in network $h$, input data feature $n_x$ and number of output node $n_y$ and returnd dictionary of parameters."},{"metadata":{"_cell_guid":"1a11f297-529e-40d1-80d7-c78d5ac9450a","_uuid":"d4ce125c21531d4e97fd698c5a6409191cd67d9c","collapsed":true,"trusted":true},"cell_type":"code","source":"def initialize_param(h, n_x, n_y):\n    # Intialize required paramters w's and b's\n        #h: number of activations\n        #n_x: number of input feature set\n        #n_y: number of output node\n    np.random.seed(1)\n    Waa = np.random.randn(h,h)\n    Wax = np.random.randn(h,n_x)\n    Wya = np.random.randn(n_y,h)\n    ba = np.random.randn(h,n_y)\n    by = np.random.randn(1,1)\n    parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n    return(parameters)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f1d910d-896a-49b2-83b6-75d0b8685d85","_uuid":"e5cde82b57d7da30578b5f5cfa4e9fbfe4bfa051"},"cell_type":"markdown","source":"**compute_cost:** The compute cost function, calculate loss of each of the training example in the data set by comparing difference between actual and produce activation from the network and then average all the loss to get a single cost value."},{"metadata":{"_cell_guid":"01b1f0d7-b797-4ff3-bcc8-0b123162f1d9","_uuid":"527f5b2e3c6e05735219ff37ec4c786df9534e8c","collapsed":true,"trusted":true},"cell_type":"code","source":"def compute_cost(y, y_pred, t_x, m):\n    #calculate logistic cost w.r.t. prediction yhat and actual value y\n        #y: target label\n        #y_pred: predicted values from network\n        #t_x: length of time stemp\n        #m: number of trainig examples\n    y_hat = y_pred[:,:,t_x-1]\n    loss = -1*(y* np.log(y_hat) + (1-y) * np.log(1-y_hat))\n    cost = np.sum(loss)/m\n    return(cost)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b9a98a5-e442-40a3-a2c7-44914aebead7","_uuid":"1d91a8ceb1d65dc74e17bcd8248948d41c966597"},"cell_type":"markdown","source":"## Backward Propagation:"},{"metadata":{"_cell_guid":"17c89547-a122-4827-be19-acb924fc746d","_uuid":"b436bee71a0e421d0859fee901ff6a738e7d3b04"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/rnn/5.jpg?raw=true)"},{"metadata":{"_cell_guid":"3f527144-9f9b-41da-9fbb-e667bd6e4d82","_uuid":"3cc1011945d1618aad7c66de399278cec065953c","collapsed":true},"cell_type":"markdown","source":"Refer to above figure, it can be observed that in the back-propagation, we divide process into two catigories:\n1. Deriavtive of output layer \n2. Derviative of RNN-Cell\n\nIn, **Deriavtive of output layer**, we will compute derivatives of parameters related to output layer, in our case $b_y$ and $W_{ya}$ are the parameters, we are considering. The equations to take derivative of parameters w.r.t loss are as follows:"},{"metadata":{"_cell_guid":"f25f8439-4d82-47e0-9067-e7ad5a47930a","_uuid":"b1af325a3d07daf53dc7eb181deecfa2529725ab"},"cell_type":"markdown","source":"$$\\frac{\\partial{loss}}{\\partial{z_y}} = \\hat{y}-y$$\n$$d_{z_y} = \\frac{\\partial{loss}}{\\partial{z_y}}$$\n$$\\frac{\\partial{loss}}{\\partial{b_y}} = d_{z_y}$$\n$$\\frac{\\partial{loss}}{\\partial{W_{ya}}} = d_{z_y} * a^{<tx>}$$"},{"metadata":{"_cell_guid":"07683319-a522-4a91-974b-9d59a3cb199b","_uuid":"7cafa313b69bad2969c9cbe408493d9e384e86b3"},"cell_type":"markdown","source":"**derivative_output: ** The derived equations above are computed in this helper funciton. The primarly task of this helper functions is to get derivatives of parameters that are connected to output layer i.e. $W_{ya}$ and $b_y$"},{"metadata":{"_cell_guid":"34ac4152-065e-454d-be03-4a803019904d","_uuid":"7cd3dd1ff1fb20c17134c37013a0165815136933","collapsed":true,"trusted":true},"cell_type":"code","source":"def derivative_output(yhat, y, a, t_x, m):\n     ## compute partial derivative of parameters at output layer\n        # yhat: activation from sigmoid \n        # y: target label\n        # a: activation at last time stamp\n    dZy = yhat - y\n    dby = (1 / m) * np.sum(dZy, axis=1, keepdims=True)\n    dWya = (1 / m) * np.dot(dZy, a[:,:,t_x-1].T) \n    \n    gradient_output = {'dWya': dWya, 'dby': dby, 'dZy': dZy}\n    return(gradient_output)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed702df3-17f5-4ad5-9cc0-43d7f9f8616c","_uuid":"d9c5153f085c6d675008afc353d7d08abdf35c3a"},"cell_type":"markdown","source":"The RNN-Cell contains three parameters $W_{aa}$, $W_{ax}$ and $b_a$, we want to update in **Derviative of RNN-Cell** process. The following are the equations for taking derivatives of parameters:\n"},{"metadata":{"_cell_guid":"75857a74-2f21-43e1-8cba-2eb3ac8df6e9","_uuid":"1841e662b246df78ae326c864082e042b2310f1e"},"cell_type":"markdown","source":"$$d_{tanh} = (1-a^{<tx>^2})*da^{<tx>} $$\n$$d_z = d_{tanh}$$\n$$d_{ba} = d_{tanh}$$\n$$d_{Waa} = d_{tanh} * a^{<tx-1>} $$\n$$d_{Wax} = d_{tanh} * x^{<tx>}$$"},{"metadata":{"_cell_guid":"45686bec-5e35-463e-91e9-c7af76dabf92","_uuid":"7abb108f494bef9173445613c85ba080d94ee065"},"cell_type":"markdown","source":"The $da^{<tx>}$ will be derived differently. This is generally, equated in last time stamp state and the rest of the early states.\n1. For last time stamp: $da^{<tx>} = W_{ya} * d_{zy}$\n2. And for early states:  $da^{<tx>} = W_{aa} * d_{z} ^{<tx+1>}$"},{"metadata":{"_cell_guid":"79a29a49-9a80-4162-9481-c964fa65f8e7","_uuid":"05df6860e7a90dd2f2d1c1a35939a189a89edf9f"},"cell_type":"markdown","source":"**rnn_cell_backward:** The paritial derivative of loss w.r.t  parmaeters in the rnn cell are computed in this function. In this case the parameters are $W_{aa}$, $W_{ax}$ and $b_{a}$. "},{"metadata":{"_cell_guid":"049f6a1a-52c4-46ce-b63f-e05abf466638","_uuid":"5e4a9cd066c2d37f20560b0a503d81b7c16c8fd9","collapsed":true,"trusted":true},"cell_type":"code","source":"def rnn_cell_backward(da_next, cache):\n    # compute derivatives and returns a dictonary objects of them\n        #da_next: derivative of activation of next state\n        #cache: tuple containing information from the forward pass (output of rnn_cell_forward())\n    \n    # Retrieve values from cache\n    (a_next, a_prev, xt, parameters) = cache\n    \n    # Retrieve values from parameters\n    Wax = parameters[\"Wax\"]\n    Waa = parameters[\"Waa\"]\n    Wya = parameters[\"Wya\"]\n    ba = parameters[\"ba\"]\n    by = parameters[\"by\"]\n\n    # compute the gradient of tanh with respect to a_next\n    dtanh = (1 - a_next ** 2) * da_next\n\n    # compute the gradient of the loss with respect to Wax \n    dxt = (1 / m) * Wax.T.dot(dtanh)\n    dWax = (1 / m) * dtanh.dot(xt.T)\n\n    # compute the gradient with respect to Waa\n    da_prev = (1 / m) * Waa.T.dot(dtanh)\n    dWaa = (1 / m) * dtanh.dot(a_prev.T)\n\n    # compute the gradient with respect to b \n    dba = (1 / m) * np.sum(dtanh, 1, keepdims=True)\n\n    \n    \n    # Store the gradients in a python dictionary\n    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n    \n    return gradients","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83b85ace-40fa-4e30-8d09-42f5dad54ecc","_uuid":"7e231df0f490cff536d3d79ad2757536a9c13eaa"},"cell_type":"markdown","source":"**rnn_backward:**  This helper function iterates on *rnn_cell_backward*. The flow of loop is backward from output layer to cell state at time one.  During the iteration this function also calculates $da^{<tx>}$ at output layer or early states other than output."},{"metadata":{"_cell_guid":"58dee2d8-2917-4567-b973-f506eba6816a","_uuid":"89820e9295f14a7e2b60b1342d75131f889ea3ec","collapsed":true,"trusted":true},"cell_type":"code","source":"def rnn_backward(caches, y, n_a, m, T_x, yhat, a): \n    # compute every states derivatives and return them\n        #cache: tuple containing information from the forward pass (rnn_forward)\n        #y: target label\n        #n_a: number of activation states\n        #m: training examples\n        #T_x: time stam\n        #yhat: predicted values\n        #a: activations\n        \n    # Retrieve values from the first cache (t=1) of caches \n    (caches, x) = caches\n    (a1, a0, x1, parameters) = caches[0]\n    \n    # Retrieve dimensions from x1's shapes \n    n_x, m = x1.shape\n    \n    da = np.empty((h,m))\n    \n    # initialize the gradients with the right sizes (≈6 lines)\n    dx = np.zeros((n_x, m, T_x))\n    dWax = np.zeros((n_a, n_x))\n    dWaa = np.zeros((n_a, n_a))\n    dba = np.zeros((n_a, 1))\n    da0 = np.zeros((n_a, 1))\n    da_prevt = np.zeros((n_a, 1))\n    \n    # Loop through all the time steps\n    for t in reversed(range(T_x)):\n        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step.\n        if (t == T_x-1):\n            gradient_output = derivative_output(yhat[:,:, t], y, a, T_x, m)\n\n            dZy = gradient_output['dZy']\n            Wya = parameters['Wya']\n            da = np.dot(Wya.T,dZy)\n        else:\n            Waa = parameters['Waa']\n            da = np.dot(Waa.T,da_prevt)      \n        \n        gradients = rnn_cell_backward(da, caches[t])\n        \n        # Retrieve derivatives from gradients (≈ 1 line)\n        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n        dx[:, :, t] = dxt\n        dWax += dWaxt\n        dWaa += dWaat\n        dba += dbat\n        \n    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n    da0 = da_prevt\n\n    # Store the gradients in a python dictionary\n    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba, \"dWya\": gradient_output[\"dWya\"],\n                 \"dby\": gradient_output[\"dby\"] }\n    \n    return gradients","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0b6bfca9-f047-46f8-81e0-319cd84d5075","_uuid":"337a01dddff23783d5174361f1469cd0395b32a6"},"cell_type":"markdown","source":"**update_parameters:** This function update paramaters w's and b's according to the following equation:\n$$w := w - \\partial{w} * learningRate $$\n$$b := b - \\partial{b} * learningRate $$"},{"metadata":{"_cell_guid":"2628cc81-9f89-4aa9-8f85-b06059ca92b6","_uuid":"a5257dd1fdaf76732555f4aedd6e103376087f08","collapsed":true,"trusted":true},"cell_type":"code","source":"def update_parameters(parameters, gradients, lr):\n    # update and return parameters \n        # parameters: dictonary object of w's and b's\n        # gradients: dictionary: object of gradient of w's and b's\n        #lr: learning rate\n        \n    parameters['Waa'] = parameters['Waa'] - (lr * gradients['dWaa'])\n    parameters['Wax'] = parameters['Wax'] - (lr * gradients['dWax'])\n    parameters['Wya'] = parameters['Wya'] - (lr * gradients['dWya'])\n    parameters['ba'] = parameters['ba'] - (lr * gradients['dba'])\n    parameters['by'] = parameters['by'] - (lr * gradients['dby'])\n    \n    return(parameters)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3107e514-aa50-4df6-86b9-a12800da3287","_uuid":"c307ff613af5846003708d7ee6ec4639ce3a4c43"},"cell_type":"markdown","source":"**update_prev_a:** The activations initalized randomnly before first time stamp is also updated as:\n$$a_0 = a_0 -\\partial{a_0} * learningRate$$"},{"metadata":{"_cell_guid":"ef447401-c324-45ae-babc-737e636727fd","_uuid":"823c479003d0d6bd01570a4f137864fdc19297ba","collapsed":true,"trusted":true},"cell_type":"code","source":"def update_prev_a(a0, gradients, lr):\n    # update and return activations\n        #a0: activations before first state\n        #gradients: dictionary: object of gradient of w's and b's\n        #lr: learning rate\n        \n    a0 = a0 - (lr*gradients['da0'])\n    return(a0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"10309c3a-f369-418f-b58d-d5f096291570","_uuid":"98ae5b6977f49d4df3ce1eba3d35dc1ca90aa1e4"},"cell_type":"markdown","source":"**optimize_parameters: ** this function call all the function i.e. the functions used in forward propagation, used in getting cost and used in back propagation to determine derivatives of paratmeters. and then update the parameters.\nThe above process run $n$ times specified by the user. "},{"metadata":{"_cell_guid":"814dbec2-d5cf-429c-996e-18c81e2278fe","_uuid":"9ed2459ff7469f908725e7a940a86b9f673034a7","collapsed":true,"trusted":true},"cell_type":"code","source":"def optimize_parameters(x, y, a0, parameters, h, m, t_x, lr, num_iter):\n    # returns list of cost at each iteration and updated parameters\n        #x: input data\n        #y: target labels\n        #a0: activation before state 1\n        #parameters: dictionary objects of parameters\n        #h: number of node\n        #m: number of training examples\n        #t_x: time stamp (length of sequence of data)\n        #lr: learning rate\n        #num_iter: an integer\n    lst_cost = []\n\n    for i in range(0, num_iter):\n        a, yhat, caches = rnn_forward(x, a0, parameters)\n        gradients = rnn_backward(caches, y, h, m, t_x, yhat, a)\n        parameters = update_parameters(parameters, gradients, lr)\n        cost = compute_cost(y, yhat, t_x, m)\n        lst_cost.append(cost)\n        a0 = update_prev_a(a0, gradients, lr)\n    return (lst_cost, parameters)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04829736-ca24-4924-8ce2-b9e8a08a0c63","_uuid":"53d8b04af131000bf105166741782c6ca27155b3","collapsed":true,"trusted":true},"cell_type":"code","source":"def plt_res(lst, ylab, lr):\n    #This will plot the list of values at y axis while x axis will contain number of iteration\n    #lst: lst of action/cost\n    #ylab: y-axis label\n    #lr: learning rate\n    plt.plot(lst)\n    plt.ylabel(ylab)\n    plt.xlabel('iterations')\n    plt.title(\"Learning rate =\" + str(lr))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a39442a-a892-4466-aef8-2818fed805dd","_uuid":"fae8bf43d14093cc2cb705cdfa53c96392dc3653"},"cell_type":"markdown","source":"We have created two different datasets containing $10000$ and $100000$ training examples with sequence lenth $50$. The fixed RNN architecture is trained on both of the datasets and their cost over time is determined.  The network contaion $25$ hidden states $h$ with learning rate $0.07$. It is trained about $1000$ iterations."},{"metadata":{"_cell_guid":"f1bf9989-e94c-49d4-aed6-cd720588e2fc","_uuid":"d8c4edce4416cdd93940b064742c653ed4d94720","collapsed":true,"trusted":true},"cell_type":"code","source":"np.random.seed(1)\n\nn_x = 1\nt_x = 50\nn_y = 1\nm = 10000\nlr = 0.07\nh = 25 \nnum_iter = 1000\n\nx = generate_bits(m, t_x)\ny = generate_label(x.T, m)\nx = x.T.reshape(n_x,m,t_x)\n\na0 = np.random.randn(h,m)\nparameters = initialize_param(h, n_x, n_y)\n\n\"\"\"Uncomment below line of code to trian RNN\"\"\"\n# lst_cost_s, parameters_s = optimize_parameters(x, y, a0, parameters, h, m, t_x, lr, num_iter)\n\n######################################################################################\n\nm = 100000\n\nx = generate_bits(m, t_x)\ny = generate_label(x.T, m)\nx = x.T.reshape(n_x,m,t_x)\n\na0 = np.random.randn(h,m)\nparameters = initialize_param(h, n_x, n_y)\n\n\"\"\"Uncomment below line of code to trian RNN\"\"\"\n# lst_cost_m, parameters_m = optimize_parameters(x, y, a0, parameters, h, m, t_x, lr, num_iter)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9af2976e-8542-4740-9dd3-8885b84b2e4a","_uuid":"56cf1d10095265eae302c1c16ea3ba4ed49b7865"},"cell_type":"markdown","source":"We have trained rnn on $10,000$ and $100,000$ training samples on local machine and the cost graphs are shown below,The x-axis shows the cost of training while y-axis plot the number of iteration:"},{"metadata":{"_cell_guid":"32b65620-5a53-490e-b7d0-5fefcc2fac46","_uuid":"af293b77fe2051e39c326dfd55b124b1f243a96c","collapsed":true},"cell_type":"markdown","source":"\nTraining Examples|Cost Graph|\n------------- |:-------------:|\n**10,000** | ![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/rnn/rnn-10000-1000-007.png?raw=true)|\n**100,000** | ![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/rnn/rnn-100000-1000-007.png?raw=true)|\n"},{"metadata":{"_uuid":"eaf693cb6ad1ddb2142f2241857fb1900c68e256"},"cell_type":"markdown","source":"\n\n---"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}