{
  "cells": [
    {
      "metadata": {
        "_uuid": "2dcb87dea6b37040a1c675a3636fc969b2ea0996",
        "_cell_guid": "2e8bb09f-4bc1-4a12-b9d3-8dbda85f004d"
      },
      "cell_type": "markdown",
      "source": "# Understanding Derivation in Context of Logistic Regression:\nThis notebook is made for the purpose of refreshining or learning the concept  of derviaton/partial derviation and use these concepts to apply at Regression model, and demonstrate simple approach to understand how excatly the Regression learns to predict the actual output. The work contains three steps as follows:\n1. Concepts of Derivation\n2. Walking through step by step the Regression w.r.t its parameters update with Gradient Update\n3. Discuss convergance of Model."
    },
    {
      "metadata": {
        "_uuid": "89f290cad2f9d2929281bc3a1ac2ef0be1e06cf6",
        "_cell_guid": "a5b71877-e08b-4959-a9fd-b2e063261c53"
      },
      "cell_type": "markdown",
      "source": "## Derivation:\nWe may say derivation is the \"fancy\" word for the concept slope of the line. The difference in between is that in derivation you are considering the change in slope values when there is infinitestimal change(infintiy small change). \nTo understand how derivatives work, we have provided two examples of addition and multiplication of function that have tow variables(x,y). Let's consider addition first. Below is solved example of addition:\n\n![Partial Derivate for Addition](https://image.ibb.co/iB5aqx/par_d_add.jpg)\n\nWhen we are dealing with two vairables, then we take derivatives in flow like we consider one variable as constant and take derivative w.r.t to other variable (Known as partial derivative). In the below figure we first take partial derivate w.r.t x and then y and given intuation how equation is solved:\n\n![Description of Addition](https://image.ibb.co/eAzLOH/par_d_add.jpg)\n\nNow, let's take example of Multiplication. The same process is followed as in the case of addition above i.e. we first consider x to take partial derivative and y.  The equation and its explanation is shown in figure below:\n![Partial Derivate for Multiplication](https://image.ibb.co/hLgFqx/par_d_mul.jpg)\n\n![Description of Addition](https://image.ibb.co/ndgOcc/par_d_mul.jpg)\n\n\n\n"
    },
    {
      "metadata": {
        "_uuid": "b6d31441bc22404f65591a07a5b7e040a1729089",
        "_cell_guid": "abd313f0-055f-48bd-9880-26455d4f95b4"
      },
      "cell_type": "markdown",
      "source": "### Computational Graph and Chain Rule:\nSo far we have considered simple equations with only two variables, what if we do have equation of more than two vaiables? \n\\begin{equation*}\nh(a, b, c) = (a * b) + c\\\\\n\\end{equation*}\nTo proceed with equations, we use computation graph. The structure of graph is simple, we place variables at input positions and operators in between the graph. And the function output is achieved at the end of graph. This will be more clear viewing below figure:\n![Forward Pass](https://image.ibb.co/m1Y3cc/Capture.jpg)\nIn figure above, it can easily be seen that how mathematical operations are perfromed in sequence steps; like a and b are  first multiplied and the result of them is added to c. We may give any name to intermidate state, that might be helpful in derivation steps. Below is the sort equation we can form:\n\\begin{equation*}\nh(a, b, c) = (a * b) + c\\\\\n(a * b) = tmp \\\\\nh(a, b, c) = tmp + c\\\\\n\\end{equation*}\n\n\nNow let's take derivate; but this time we will start operation from right to left, considering each operation step by step. The first step would be working on '+' operation and then 'multipy'.  The following is calculation for partial deriates of function 'h' w.r.t. variables 'a, b, and c'.\n\n\n\\begin{equation*}\n\\frac{\\partial h}{\\partial c} = \\frac{\\partial h}{\\partial c}(tmp +c)\\\\\n\\frac{\\partial h}{\\partial c} =1\\\\\n\\frac{\\partial h}{\\partial tmp} = \\frac{\\partial h}{\\partial tmp}(tmp +c)\\\\\n\\frac{\\partial h}{\\partial tmp} =1\n\\end{equation*}\n\n\nFor determining partial derivative of a and b we will be using chain rule. And it states that multiply all previous derivates to new. Thus the partial dervatives of a and b will be computed as follows\n\\begin{equation*}\n\\frac{\\partial h}{\\partial a} = \\frac{\\partial h}{\\partial tmp}* \\frac {\\partial tmp}{\\partial a}\\\\\n\\frac{\\partial h}{\\partial b} = \\frac{\\partial h}{\\partial tmp} * \\frac{\\partial tmp}{\\partial b}\\\\\n\\end{equation*}\n\nThe values of paritial derivate of h w.r.t 'a' and 'b' are computed as follows:\n\n\\begin{equation*}\n\\frac{\\partial h}{\\partial a} = \\frac{\\partial h}{\\partial tmp}* \\frac {\\partial tmp}{\\partial a}\\\\\n\\frac{\\partial tmp}{\\partial a} = \\frac{\\partial tmp}{\\partial a}(a * b)\\\\\n\\frac{\\partial tmp}{\\partial a} = b\\\\\n\\frac{\\partial h}{\\partial a} = 3 * 1 = 3 :[{\\partial a} = 3; {\\partial tmp} = 1]\\\\\n\\end{equation*}\n\n\\begin{equation*}\n\\frac{\\partial h}{\\partial b} = \\frac{\\partial h}{\\partial tmp} * \\frac{\\partial tmp}{\\partial b}\\\\\n\\frac{\\partial tmp}{\\partial b} = \\frac{\\partial tmp}{\\partial a}(a * b)\\\\\n\\frac{\\partial tmp}{\\partial b} = a\\\\\n\\frac{\\partial h}{\\partial b} = 2 * 1 = 2 :[{\\partial a} = 2; {\\partial tmp} = 1]\\\\\n\\end{equation*}\n\nThe diagram of derivatives below can validate the above concepts.\n![](https://image.ibb.co/gS3iTS/Capture.jpg)\n\nYou may find further resources about derivation containing rules, calculator of derivator and more examples of derivation  in links below:\n\n* [CS231n Backpropagation](http://cs231n.github.io/optimization-2/)\n* [Derivate Rules](https://www.mathsisfun.com/calculus/derivatives-rules.html)\n* [Derivation Calculator](https://www.derivative-calculator.net/)\n\n**I would highly recommend to have a look on the resources and also try to understand sigmoid function presented in the CS231n Backpropagation.**"
    },
    {
      "metadata": {
        "_uuid": "b714044c448946ddd9f0ea6ed1707656d5c720c7",
        "_cell_guid": "9b4453e0-8041-4754-9b49-9e7c64bd4374"
      },
      "cell_type": "markdown",
      "source": "## Regression ##\nIn this section, the regression is shown in Computational Graph to described how to compute loss and using that loss calculate gradient and update its wight.  For sake of simplicity we have considered only one training example with two feature set i.e. two inputs x1, x2. These input values are then used in xor function to generate output  (target value 'y'). The generated output value 'y' is used to calculate the loss by comparing this value to the activation generated through regression computation graph.\n"
    },
    {
      "metadata": {
        "_uuid": "117927782c52fcf6596f1f53149960eafce74e15",
        "_cell_guid": "e18bfb00-dc58-4410-8c4e-5c263f06f7cd"
      },
      "cell_type": "markdown",
      "source": "In the following computational graph, the two feature set i.e. *x1* and *x2* are multiplied by weights *w1* and *w2* and their product is shown in *d1* and *d2* states. The bias *b* is  added to both *d1* and *d2* state yielding the z state. To get the activation of *z*; the sigmoid function is applied and finally the loss is computed with actual value of xor function (using same values of x1 and x2) *y* and the activation value *a*. We may say this whole process as **Forward Pass**.\n![](https://image.ibb.co/cqENsn/Capture.png)"
    },
    {
      "metadata": {
        "_uuid": "2d99f29bf7650f0d91a5e483d2f0596fc2d3ec0e",
        "_cell_guid": "923c58dd-f7a0-49ad-8a5e-27830fc8fc03"
      },
      "cell_type": "markdown",
      "source": "let's implement the above graph with using random generated weights and bias and see how activation is different from the actual value.\n"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# This is Python 3 environment \n# Call packages for onward use\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # ploting graph",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def sigmoid(z):\n    # Takes input as z and return sogmoid of value\n    s = 1 / (1 + np.exp(-z))\n    return s",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "_uuid": "abf512f1a3532897f26a5e55555c448f99cba37d",
        "_cell_guid": "55e4dbc7-92d8-4695-a845-a5423e4b8f53",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# gnerate data with two feature set and get label of xor function\nx1, x2 = 1, 1\ny = int(np.logical_xor(x1,x2))\nprint('actual value(y): ', y)\n\n# define paramters i.e. weights and bias to random\nw1, w2, b = 0.1, 0.5, 0.005\n\n# print('Parameters Before update')\n# print('w1: ', w1, 'w2: ', w2, 'b: ', b)\n\nz = w1*x1 + w2*x2 + b\n\n#activation of values \na = sigmoid(z)\nprint('activation value: ', a)\n\n# compute the loss of the function(since we have training exmple equal to 1, so cost==loss )\ncost = -1 * (y * np.log(a) + (1 - y) * (np.log(1 - a)))  # compute cost\nprint('loss of function: ', cost)\n\n# Store cost, activation, weights and bias to dictionary\nmy_dic = {}\nmy_dic['w1'] = w1\nmy_dic['w2'] = w2\nmy_dic['b'] = b\nmy_dic['activation'] = a\nmy_dic['cost'] = cost\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ab1a7036e1a6842e713d1700d5e8b7148f67e1a5",
        "_cell_guid": "d5c243e2-6df3-4688-b540-65e4b922ae28"
      },
      "cell_type": "markdown",
      "source": "We can see that the actual value is *0* is far away from the activation value *0.65* and cost of this function is much higher. Now let's see how gradient upadate the weight and affects the loss(reduce) to get more closer value of activation to the actual value.  Before implementing code, let's do some maths stuff to see how greadients of weights & bias accroding to loss can be calcalulated. \n\nIn this step, we will go from right to left opposite to Forward pass and we may say it **Backward Pass**"
    },
    {
      "metadata": {
        "_uuid": "4a61e9f712efa795dd4f7d302467cb0e5ea0862e",
        "_cell_guid": "d52a89eb-51f1-45c7-87f6-eb3e64ca5a96"
      },
      "cell_type": "markdown",
      "source": "![](https://image.ibb.co/m2Zsa7/Capture.jpg)"
    },
    {
      "metadata": {
        "_uuid": "b61267154495890462c02c0645e0bc9d3de553c2",
        "_cell_guid": "8dbf4bc2-447c-423a-8acc-24bea67b2b9f"
      },
      "cell_type": "markdown",
      "source": "\\begin{equation*}\n\\frac{\\partial loss}{\\partial a} = \\frac{-y}{\\ a} - \\frac{1-y}{1- a}   :>[da] \\\\\n\\frac{\\partial loss}{\\partial z} = \\frac{\\partial loss}{\\partial a} * \\frac{\\partial a}{\\partial z} :>[dz]\\\\\n\\frac{\\partial a}{\\partial z} = (1-a)*a <refer-to: cs231n>\\\\\n\\frac{\\partial loss}{\\partial z} = (a-y) :>[dz]\\\\\n\\frac{\\partial loss}{\\partial b} = \\frac{\\partial loss}{\\partial z} * \\frac{\\partial z}{\\partial b} :>[db]\\\\\n\\frac{\\partial z}{\\partial b} =1<sum-rule>\\\\\n\\frac{\\partial loss}{\\partial b} = (a-y) :>[db] ...(1)\\\\\n\\frac{\\partial loss}{\\partial d1} = (a-y) :>[dd1]\\\\\n\\frac{\\partial loss}{\\partial d2} = (a-y) :>[dd2]\\\\\n\\frac{\\partial loss}{\\partial w1} = \\frac{\\partial loss}{\\partial d1} * \\frac{\\partial d1}{\\partial w1} :>[dw1]\\\\\n\\frac{\\partial d1}{\\partial w1} = x1<product-rule> \\\\\n\\frac{\\partial loss}{\\partial w1} = x1 * (a-y) :>[dw1]...(2)\\\\\n\\frac{\\partial loss}{\\partial w2} = x1 * (a-y) :>[dw2]...(3)\\\\\n\\end{equation*}"
    },
    {
      "metadata": {
        "_uuid": "a27c363dfaad627bb1b706927a37c6b3adc2a000",
        "_cell_guid": "98bc711e-68ed-4c40-ad16-2b1d2dfc4b2f"
      },
      "cell_type": "markdown",
      "source": "The Equation no 1, 2 and 3 are equations above; determines the gradients of weights and bias w.r.t loss. Their code implementation is given below:"
    },
    {
      "metadata": {
        "collapsed": true,
        "_kg_hide-input": false,
        "_uuid": "4dac4bd02facd3f311ba05fe55e5a29cbf4726ca",
        "_cell_guid": "4b004365-11fa-4ec0-b3b8-9e0471e65d5a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# BACKWARD PROPAGATION (TO FIND GRAD)\ndw1 = x1*(a-y)\ndw2 = x2*(a-y)\ndb = a - y ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "3e50224ab131c001b9d576ceabd6623a6ab874db",
        "_cell_guid": "4c1ddf76-fe88-417f-b61e-c7c7b27ec835"
      },
      "cell_type": "markdown",
      "source": "The gradients of weights and bias are used to update the initial weights and bias. In this step we multiply gradient with a learning rate(this controls the graident upadate contribution in the parameters weights and bias).  Why we subtract the product of gradient and learning rate from the previous parameters? following video of Prof. Andrew Ng will give you more intuation about the calculations:"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "bc43e1d7f632eec4da0f8f1336da8f27892a9aa1",
        "_cell_guid": "7f8b2b32-83b2-4e60-a04d-10f648c4fcd0",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from IPython.display import HTML\n\n# Youtube\nHTML('<iframe width=\"800\" height=\"400\" src=\"https://www.youtube.com/embed/8mS1DlibKbI\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b2013cfcb13b6c1e604ac1d500ab98bc3c9d1d17",
        "_cell_guid": "3e0f6c8f-2fd8-4396-94e4-95554a6b3818",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# update parameter w1, w2 and b by the equation\n\nlr =0.07 # learning rate\n\nw1 = w1 - (lr*dw1)\nw2 = w2 - (lr*dw2)\nb = b - (lr*db)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40a6f85f9847bc9753c1fb00bbb56c2429599d99",
        "_cell_guid": "be06b881-d567-4ed8-8284-90744c3877c5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "z = w1*x1 + w2*x2 + b\n\n#activation of values \na = sigmoid(z)\nprint('activation value before update: ', my_dic['activation'])\nprint('activation value after update: ', a)\nprint('')\n\n# compute the loss of the function(since we have training exmple equal to 1, so cost==loss )\ncost = -1 * (y * np.log(a) + (1 - y) * (np.log(1 - a)))  # compute cost\nprint('loss of function before update: ', my_dic['cost'])\nprint('loss of function after update: ', cost)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "05ebc8ec1369554245f92deb6dbd9b79368ca742",
        "_cell_guid": "9ac9fd39-e5db-45d5-a494-626e576fef59"
      },
      "cell_type": "markdown",
      "source": "From the above result we can easily recognize the change in activation and loss in one step **Backward Pass**. Before performing update in weights, we are getting higher loss while in one step update we yield in smaller loss than before. Let's do update multiple time to see how the weight and bias update effect the loss. "
    },
    {
      "metadata": {
        "_uuid": "9d02d131b72585e9bc2bd9050a6aef66d8e21e73",
        "_cell_guid": "1c3d8137-7b56-423a-b1d1-95acd544f5b6"
      },
      "cell_type": "markdown",
      "source": "### Update Paramters w and b multiple times:\nIn this section we will update pramaters weights and bias multiple times by looping over the process we have followed above and see the results, following are the steps:\n1. Define the function that will compute activation and loss\n2. Define the function to update the paramaters\n3. Define the function plot the results\n4. iterate over the above function multiple times and plot the results"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "5d9183a4c49e0eadca2cc7e4456424b23296b913",
        "_cell_guid": "6082c054-d6fb-463a-98c0-c22a72ad2860",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def get_activation_loss(x1, x2, w1, w2, b):\n    # this function compute activations, cost and z\n        # x : input features\n        # w : weight\n        # b : bias\n    z = w1*x1 + w2*x2 + b\n\n    #activation of values \n    a = sigmoid(z)\n\n    # compute the loss of the function(since we have training exmple equal to 1, so cost==loss )\n    cost = -1 * (y * np.log(a) + (1 - y) * (np.log(1 - a)))  # compute cost\n    \n    return(a,cost, z)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "f7fd6741debcc405dcf2a5b5e513769b1b40ad2e",
        "_cell_guid": "677270f6-b9f0-47bc-b84e-cafcea49ba47",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def update_paramters(x1, x2, w1, w2, b, a, y, lr):\n    # This function computes gradient of parmaters and then update them\n    # returns upadated parameters weights and bias\n        # x: input features\n        # w: weights\n        # b: bias\n        # a: activation\n        # y: actual label\n        # lr: learning rate\n      \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    dw1 = x1*(a-y)\n    dw2 = x2*(a-y)\n    db = a - y \n\n    # update parameter w1, w2 and b by the equation\n\n    w1 = w1 - (lr*dw1)\n    w2 = w2 - (lr*dw2)\n    b = b - (lr*db)\n    \n    return(w1, w2, b)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "d5b01dde8da21cba4004654c2c9431a170465fac",
        "_cell_guid": "49d1fa40-ec3b-4b3a-a3f3-58158b25e612",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def plt_res(lst, ylab, lr):\n    #This will plot the list of values at y axis while x axis will contain number of iteration\n    #lst: lst of action/cost\n    #ylab: y-axis label\n    #lr: learning rate\n    plt.plot(lst)\n    plt.ylabel(ylab)\n    plt.xlabel('iterations')\n    plt.title(\"Learning rate =\" + str(lr))\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2c756395dec9324f618d5392b24ec547cc87884",
        "_cell_guid": "981d5e84-a69d-468c-8258-6b7d9f3f3075",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# define paramters i.e. weights and bias to random to new random values\nw1, w2, b = 0.04, 0.75, 0.0015\nlst_cost = []\nlst_activation = []\n#In code below, we will update paramets about 3500 times.\nnum_iter = 3500\nlr = 0.007\n\n# gnerate data with two feature set and get label of xor function\nx1, x2 = 1, 0\ny = int(np.logical_xor(0,1))\n\nprint ('x1: ', x1, '; x2: ',x2)\nprint('xor value(y): ', y)\n\nfor i in range(num_iter):\n    a,cost,z = get_activation_loss(x1, x2, w1, w2, b)\n#     print('cost at iteration', i,': ', cost)\n#     print('activation at iteration', i,': ', a)\n    w1, w2, b = update_paramters(x1, x2, w1, w2, b, a, y, lr)\n    lst_cost.append(cost)\n    lst_activation.append(a)\n\nplt_res(lst_cost, 'loss', lr)\nplt_res(lst_activation,'activation', lr)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c52f531857324d7302f6a5dafd8a30373af7815",
        "_cell_guid": "1331f768-26b8-46df-9432-4cc86b03da78"
      },
      "cell_type": "markdown",
      "source": "In the above graphs, we have plotted loss and values to y axis with iteration on the x-axis.  It can be seen easily that in initial iteration steps we are having activation values that are far away from the actual value y thus resulting in higher loss value. As we go through updating the parameters weights and bias we are achiving activation values closer to y i.e. 1 and also error near to 0.\n"
    }
  ],
  "metadata": {
    "language_info": {
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}