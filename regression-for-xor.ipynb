{"cells":[{"metadata":{"_cell_guid":"4b707e2d-9783-4f79-8a46-93d3a32cba43","_uuid":"c9d79ec0886b7f50d1e33bce671aa652762ae2c8"},"cell_type":"markdown","source":"# Regression for XOR:\nThis notebook is based on the concept of [derivation in context of Logistic Regression](https://www.kaggle.com/hamzafar/derivation-in-context-of-logistic-regression). In this notebook we will extend the the beforementioned work and generalized it to *m* number of rows and *n_x* feature set (columns). The folow of notebook is as follows:\n1. A function will generate data set of desired length and width (row, columns)\n2. The labelled is created using *XOR*; the deata genereted in the above step then passed to this step to get the value of *XOR* operation\n3. Implement Regression model (Generalized to deal with dynamic data set\n4. Discuss the performance Regression on different dataset and different parameters."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # ploting graph\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"799a8c28-2c50-4ce0-8c0c-2f8490cebfad","_uuid":"126f813229568a4300f7895fac877f17c47b8d6d"},"cell_type":"markdown","source":"### Generate Data\nTo discuss Generalized behavior of Regression model, by Generalized we mean it can work on any shape of data, we have created the *generate_bits* functions. The function is simple as it takes desired number of rows *m* and number of feature *n_x* and it randomly generated binary data i.e. *0* and *1*."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"def generate_bits(n_x, m):\n# Generate a m x n_x array of ints between 0 and 1, inclusive:\n# m: number of rows\n# n_x : number of columns per rows/ feature set\n    np.random.seed(1)\n    data = np.random.randint(2, size=(n_x, m))\n    return(data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"846180c8-ee33-4079-9d3b-641486c325f9","_uuid":"56ca0eff3dc3ea9043183b2db59dff314c943d31"},"cell_type":"markdown","source":"### Create Labels:\nFor training/updating derivatives of parameters weight and bias, the loss function determine the difference between actual values and the activation values. The actual value is the value that each example(row) has as it label. Like the the actual value of *OR* operation:\n\n\n\n\n\\begin{equation*}\n1 + 0 = 1 =actualValue >:[or_{operation} = +]\\\\\n\\end{equation*}\n\nThe *generate_label* function below takes data as input and apply *XOR* operation row wise.\n"},{"metadata":{"_cell_guid":"af143906-bae0-49a2-9854-556b19406e60","_uuid":"f79a134eaee498bc866b4318a99524054ec5d5cc","collapsed":true,"trusted":true},"cell_type":"code","source":"def generate_label(data, m):\n    # generate label by appyling xor operation to individual row\n    # return list of label (results)\n        # data: binary data set of m by n_x size\n    lst_y = []\n    y= np.empty((m,1))\n    k = 0\n    for tmp in data.T:\n        xor = np.logical_xor(tmp[0], tmp[1])\n\n        for i in range(2, tmp.shape[0]):\n            xor = np.logical_xor(xor, tmp[i])\n    #     print(xor)\n        lst_y.append(int(xor))\n        y[k,:] = int(xor)\n        k+=1\n    return(y.T)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"36e5ccdf-7efb-4712-9128-f8663e05d09b","_uuid":"a8f57d93f1c271cee4ae28b6f4e407d11b3775b9"},"cell_type":"markdown","source":"### Regression (Genearlized to m by n_x data-set):\nIn [derivation in context of Logistic Regression](https://www.kaggle.com/hamzafar/derivation-in-context-of-logistic-regression) we have created computational graph using only tow input features *x1* and *x2*. Now we are generalizating the pervious concept with feature length equal to *n_x* . The computational graph for this generalization would be same as before but a minor change in the input to graph i.e. the *x's, w's*. The bias *b* will be only single value. Refer to following figure the concept is described:\n![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Regression_for_XOR/1.jpg?raw=true)"},{"metadata":{"_cell_guid":"0c7c175b-c26f-48bd-a032-b3291ce49ec8","_uuid":"d32878e43ca27e511b113a498ea1d07b46d9315c"},"cell_type":"markdown","source":"To synchronize with the computational graph above, we will arrange dataset in the format that can easy fits with it. So,  we arrange dataset into *n_x by m* shape. This will easily fit the above conpcet because at each step you will pass all feature values of single sample at once that will result in conveyor belt scenario. Like you first chop front edge of dataset and pass it to graph and do computation simultaneously. \nThe activation are then stacked after the sigmoid node where you will be computing loss with compare it to actual values of *XOR*. Let's consider following figure to validate the concept:\n"},{"metadata":{"_cell_guid":"ec00aee7-f3e3-4f11-98ec-8a9f80650993","_uuid":"975f4d7f6b8561a4e8887a954894c255f7e75925"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Regression_for_XOR/2.jpg?raw=true)"},{"metadata":{"_cell_guid":"892c446f-30b6-482e-93c9-5bbff02ffffa","_uuid":"fcea8f0c608e0dbbe21280fc18ef6e442bfd193f"},"cell_type":"markdown","source":"The three functions below **(i). initialize_param, (ii). sigmoid and (iii) get_activation_loss** are helper to get loss of the input data.\nThe all three function will use matrix operations to optimize computations as for loops are computationaly expensive. and we have done two major changes in **get_activation_loss** function than the   as:\n1. Instead of multiple each single input feature to respective weight, we applid matrix multiplication operation:\n\\begin{equation*}\nw1*x1 + w2*x2+ ... +w_{nx}*x_{nx} = np.dot(w.T,x)\n\\end{equation*}\n2. Each sample in *Forward Pass* will yield into a loss and we will be having m losses as of total number of sample so we compute average of loss and it is called as *cost*\n"},{"metadata":{"_cell_guid":"4440f9c2-9e91-4f1e-b95c-38f762aa03cc","_uuid":"fc3aa781420c4eb9c6bd44ab80191a57e7c5aa48","collapsed":true,"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    # Takes input as z and return sogmoid of value\n    s = 1 / (1 + np.exp(-z))\n    return s","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edd01996-cc7e-4a60-9be1-d59ee21c68a9","_uuid":"599bec6ed3ac4c2c6be51a906cf396d673a58fcf","collapsed":true,"trusted":true},"cell_type":"code","source":"def intialize_param(n_x):\n    # initialize paramaters w and b to zero and return them\n    # size of w equal to size fo feature set and b is single value\n        # n_x: size input feature    \n    w = np.zeros(shape=(n_x, 1))\n    b = 0\n    return(w,b)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db201617-6a16-4be4-99b5-36c61cbab09a","_uuid":"0bd7ffddfb4de8ae88210665cf594c1795b8eb1f","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_activation_loss(x, w, b):\n    # this function return action, cost and z values\n        # x: input data\n        # w: weights\n        # b: bias\n    z = np.dot(w.T, x) + b\n    a = sigmoid(z)\n\n    cost = (1/m) * np.sum(-1 * (y * np.log(a) + (1 - y) * (np.log(1 - a))))\n    return(a,cost, z)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"379b4b1e-170e-44a7-be55-59ce19469714","_uuid":"1a95d5481369ff154f7e9095b154ad47415c7abb"},"cell_type":"markdown","source":"The partial derivative of b and w w.r.t. loss (of single sample) was calculated as:"},{"metadata":{"_cell_guid":"fdb1b53d-bb15-44e6-a217-de5e3f02a041","_uuid":"31506493f59424667007ba0adc82ae275d9d0077"},"cell_type":"markdown","source":"\\begin{equation*}\n\\frac{\\partial loss}{\\partial b} = (a-y)\\\\\n\\frac{\\partial loss}{\\partial w} = x*(a-y)\\\\\n\\end{equation*}"},{"metadata":{"_cell_guid":"acd0657a-61dc-4f75-aaa1-62f6bd00b698","_uuid":"241545724dba50de8e4790058558bf0cb61ba689"},"cell_type":"markdown","source":"But now we are having *m* number of sample that is giving us *m* activations *a* and actual values *y* . To get single value of cost we just get take average of each sample gradient.\n\n\n\\begin{equation*}\n\\frac{\\partial loss}{\\partial b} = (a_1-y_1)\\\\\n\\frac{\\partial loss}{\\partial b} = (a_2-y_2)\\\\\n\\frac{\\partial loss}{\\partial b} = (a_3-y_3)\\\\\n...\\\\\n\\frac{\\partial loss}{\\partial b} = (a_m-y_m)\\\\\nso,\\\\\n\\frac{\\partial cost}{\\partial b} = \\sum\\limits_{i=1}^m  \\frac{1}{m} * (a-y)\\\\\n\\end{equation*}\nThe similar will be used for partial derivatives of *cost* w.r.t *w*.\n\nThe function **update_paramters** implements the above equation in matrix operation format."},{"metadata":{"_cell_guid":"9d91b681-e691-4d32-8bad-535f592f2a62","_uuid":"34293f7dddb1734aa6470bc1f2debb5ae8704232","collapsed":true,"trusted":true},"cell_type":"code","source":"def update_paramters(x, w, b, a, y, lr, m):\n    # find the gradient of paramaters and update them (w and b)\n        # x: input data \n        # w, b: parameters (w and b)\n        # a, y: activation and actual values\n        # m, lr: total number of rows, learning rate\n    dw = (1/m) * np.dot(x,(a-y).T)\n    db = (1/m) * np.sum(a - y)\n    \n    w = w - (lr*dw)\n    b = b - (lr*db)\n    \n    return(w, b)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d79c6509-6f50-4649-b5ad-da9f05e0dbfd","_uuid":"af0a0fdae59d5151e9a6a5e38ac635f6383a846b"},"cell_type":"markdown","source":"The one important thing that must be considered while taking partial derivate *cost* w.r.t to *w*, the shape of must be consist in the iteration process. So, below is the figure that give an intuation about the cycle. For example if we have sample of shape *(5,10)* and it is multiplied by matrix w *(5,1)* then the resulting would be of shape *(1,10)* and while taking partial derivative the resulting matrix would be the same shape of *w* i.e. *(5,1)*\nThe figure below descibe the cycle of updating parameter *w* while keep track of dimensions.:\n\n![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/Regression_for_XOR/3.jpg?raw=true)"},{"metadata":{"_cell_guid":"98ad8b7e-ebd7-4714-bc17-66581f50ecd6","_uuid":"8479560ab725b8f3ef18b39d6e278c9139356142","collapsed":true,"trusted":true},"cell_type":"code","source":"def plt_res(lst, ylab, lr):\n    #This will plot the list of values at y axis while x axis will contain number of iteration\n    #lst: lst of action/cost\n    #ylab: y-axis label\n    #lr: learning rate\n    plt.plot(lst)\n    plt.ylabel(ylab)\n    plt.xlabel('iterations')\n    plt.title(\"Learning rate =\" + str(lr))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e31787e1-24cf-4506-b016-e9d7c9e134fd","_uuid":"157520a322ab07851c6154cf58ac69413c684d3c","collapsed":true,"trusted":true},"cell_type":"code","source":"def optimize_paramters(x, y, w, b, n_x, lr, num_iter):\n    # this function returns upadated values of parameters and cost\n    # It first initialize parameters and update them by computing partial derivatives\n    # Then loop over \n        # x: input data\n        # y: actual values (labels)\n        # w, b: parameters\n        # n_x, lr: input feature length, learning rate\n        # num_iter: number of cycle\n    lst_cost = []\n\n    w, b = intialize_param(n_x)\n\n    for i in range(num_iter):\n        a,cost,z = get_activation_loss(x, w, b)\n#         print('cost after iteration %i: %f' %(i,cost))\n        w, b = update_paramters(x, w, b, a, y, lr, m)\n        lst_cost.append(cost)\n    \n    return(w, b, lst_cost)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d136539-67a0-4d65-a551-8f0471b23aaa","_uuid":"9c137c64642f434d3d547e6c0ad07195e73d1be6"},"cell_type":"markdown","source":"we have implemented Regression using *10000* , *100000*  and *1000000* samples and sotred their respective learned paramteres (weights and bias) and cost of the function."},{"metadata":{"_cell_guid":"2ddb212e-5a97-4bb9-9ba1-5e3069756e4e","_uuid":"4af8fc4d5f4de9ff6f780d039ab34b23fd166547","collapsed":true,"trusted":true},"cell_type":"code","source":"n_x = 50\nm = 10000\nnum_iter = 1000\nw, b = intialize_param(n_x)\nx = generate_bits(n_x,m)\ny = generate_label(x, m)\nlr = 0.07\n\n#w_s, b_s, lst_cost_s represent values when sample set is 10000\nw_s,b_s, lst_cost_s = optimize_paramters(x, y, w, b, n_x, lr, num_iter)\n##----------##\n\nm = 100000\n# num_iter = 150\nw, b = intialize_param(n_x)\nx = generate_bits(n_x,m)\ny = generate_label(x, m)\n# lr = 0.07\n\n#w_m, b_m, lst_cost_m represent values when sample set is 100000\nw_m,b_m, lst_cost_m = optimize_paramters(x, y, w, b, n_x, lr, num_iter)\n##----------##\n\nm = 1000000\nnum_iter = 15\nw, b = intialize_param(n_x)\nx = generate_bits(n_x,m)\ny = generate_label(x, m)\n# lr = 0.07\n\n#w_l, b_l, lst_cost_l represent values when sample set is 1000000\nw_l,b_l, lst_cost_l = optimize_paramters(x, y, w, b, n_x, lr, num_iter)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d42b027e-9a17-4a79-85f1-e887c8eee9f0","_uuid":"256201bfaa949e24f3074fcf1cca573095268bbd"},"cell_type":"markdown","source":"**Prediction**\nIn prediciton step, we have done following two steps:\n1.  Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction\n\nTo validate how the Regression is performing; we just created new dataset of *0.1 times m* size and computed its predictions from the **get_prediction** function. The label of created data is also generated that are matched with the prediction values to get accuracy. Since we have trained weights for two different datasets, we computed the accuracy for both."},{"metadata":{"_cell_guid":"19c5d532-1d0b-4c43-842b-3e7dfca29cb2","_uuid":"093cb5463ffa75e6035e7a420ba56bb7ef6ced0b","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_prediction(x, w, b, m):\n    # returns the prediction on the dataset\n        # x: input data (unseen)\n        # w, b: parameters weights and bias\n        # m: total sample set\n    a = sigmoid(np.dot(w.T, x) + b)\n    y_prediction = np.zeros((1, m))\n    for i in range(a.shape[1]):\n        y_prediction[0,i] = 1 if a[0, i] > 0.5 else 0\n    return(y_prediction)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4983d36d-3039-46d3-9d97-f9a155cb23c2","_uuid":"67c5f4a839c639d3544ef4f451a6a4a94996be60","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_accuracy(y, y_prediction, m):\n    # return the accuracy by calculated the difference between actual and predicted label\n        # y: actual values\n        # y_prediction: prediction acquired from the get_prediction\n        # m: total number of sample\n    df = pd.DataFrame()\n    df['actual'] = y[0]\n    df['prediction'] = y_prediction[0]\n    df['compare']= df['prediction'] == df['actual']\n\n#     print(df[df['compare']==True])\n#     print('Accuracy: ' ,len(df[df['compare']==True]['compare'])/m)\n    return(len(df[df['compare']==True]['compare'])/m)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e9e928a-a387-45fe-9203-090cc6db02b0","_uuid":"28c01b91dfd83b99c0accfab8162faaaa7a2e9ce","collapsed":true,"trusted":true},"cell_type":"code","source":"tm = int(0.1 * m)\nx = generate_bits(n_x, tm)\ny = generate_label(x, tm)\n\ny_prediction = get_prediction(x, w_s, b_s, tm)\nacc_s = get_accuracy(y, y_prediction, tm)\n\ny_prediction = get_prediction(x, w_m, b_m, tm)\nacc_m = get_accuracy(y, y_prediction, tm)\n\ny_prediction = get_prediction(x, w_l, b_l, tm)\nacc_l = get_accuracy(y, y_prediction, tm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bccf123a-ce37-46f0-8979-cb36f9c63c73","_uuid":"4e7893b1ce963b517263a6a734fa44e317772672","collapsed":true,"trusted":true},"cell_type":"code","source":"print('------- 10000 training set-------------')\nprint('Accurcy at 10000 training set: ', acc_s)\nplt_res(lst_cost_s, 'cost', lr)\n\nprint('-------100000 training set-------------')\nprint('Accurcy at 100000 training set: ', acc_m)\nplt_res(lst_cost_m, 'cost', lr)\n\nprint('-------1000000 training set-------------')\nprint('Accurcy at 1000000 training set: ', acc_l)\nplt_res(lst_cost_l, 'cost', lr)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68cd26d6-5a4c-4701-8a8a-450794e072ac","_uuid":"7cdbe86dd95f7199f14d1f8bfda28b84fa8217c4"},"cell_type":"markdown","source":"## Discussion:\nWe have used three different data set with *10000*, *100000* and *1000000*. For first two dataset we ran loop about *1000* times to update paramters and for *1000000* the loop is ran about 15 times(due to computational cost we consider loops over small number).\n\nSurperising all three dataset yielded into about same accuracy of *50%* and we were not able to increase it\n\n"},{"metadata":{"_uuid":"2d0b588b59235fe9534f1bfbacda47f6dfde9a83"},"cell_type":"markdown","source":"---"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}