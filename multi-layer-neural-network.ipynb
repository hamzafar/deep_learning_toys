{"cells":[{"metadata":{"_uuid":"aff53d13c6b42be2bea0a11cb2f97f273813cd00","_cell_guid":"ab2a8205-e8ef-4d4b-9450-657184c377b7"},"cell_type":"markdown","source":"# Multi-Layer Neural Network:"},{"metadata":{"_uuid":"0aefac0d31dfe08166f04a8041d3cad1528ad5cc","_cell_guid":"4e9080be-2949-41cc-af7b-09fb058e3dec"},"cell_type":"markdown","source":"Multi-Layer Neural Network is considered as extension to [two layer neural network](https://www.kaggle.com/hamzafar/two-layers-neural-network), we have built before. In this notebook, we will see the generic architecture of neural network as:\n1. Description of generic Neural Network\n2. Deployment of Network\n3. Performance on different legth Training Examples"},{"metadata":{"collapsed":true,"_uuid":"7c20e95be68ea96397402587d8cad601f840d432","_cell_guid":"11f5341d-e55e-4ddb-bd27-d75f401e68e9","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # ploting graph","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17964ab387ae22723f3fa3df5dde798dcdf4d156","_cell_guid":"d11eae29-88f1-4c90-8e4b-b1d1821476ec"},"cell_type":"markdown","source":"### Generate Data\nTo discuss Generalized behavior of Regression model, by Generalized we mean it can work on any shape of data, we have created the generate_bits functions. The function is simple as it takes desired number of rows m and number of feature n_x and it randomly generated binary data i.e. 0 and 1."},{"metadata":{"collapsed":true,"_uuid":"a5b9a07783a7acd7d8da4a559f608f73b3ae6af8","_cell_guid":"14b87fdd-3c45-4967-8045-4987ebf6b5e7","trusted":false},"cell_type":"code","source":"def generate_bits(n_x, m):\n# Generate a m x n_x array of ints between 0 and 1, inclusive:\n# m: number of rows\n# n_x : number of columns per rows/ feature set\n    np.random.seed(1)\n    data = np.random.randint(2, size=(n_x, m))\n    return(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38fb58a073528ef64a8bfc57a3d839f6cd34807b","_cell_guid":"71e9e9b3-fe1a-4f85-9264-779a0e9b2d87"},"cell_type":"markdown","source":"### Create Labels:\nFor training/updating derivatives of parameters weight and bias, the loss function determine the difference between actual values and the activation values. The actual value is the value that each example(row) has as it label. Like the the actual value of OR operation:\n$$1+0=1=actualValue>:[oroperation=+]$$\nThe generate_label function below takes data as input and apply XOR operation row wise."},{"metadata":{"collapsed":true,"_uuid":"a99c6e1fb71671198221871620e8f47e89554cca","_cell_guid":"020c402c-520c-43c4-a299-53072742fea7","trusted":false},"cell_type":"code","source":"def generate_label(data, m):\n    # generate label by appyling xor operation to individual row\n    # return list of label (results)\n        # data: binary data set of m by n_x size\n    lst_y = []\n    y= np.empty((m,1))\n    k = 0\n    for tmp in data.T:\n        xor = np.logical_xor(tmp[0], tmp[1])\n\n        for i in range(2, tmp.shape[0]):\n            xor = np.logical_xor(xor, tmp[i])\n    #     print(xor)\n        lst_y.append(int(xor))\n        y[k,:] = int(xor)\n        k+=1\n    return(y.T)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a91371038b4d43230621260da9b44422368f062","_cell_guid":"a079f6dc-b83b-48b9-9896-a64fe101d06f"},"cell_type":"markdown","source":"## Multi Layer Neural Network:\n### Forward Propagation:\nIn Multi Layer Neural Network, commonly known as Multi Layer Perceptrons (MLP), the input and out layer are dependent on the input feature *n_x* and targe label *y* but the hidden layer consiting of hidden units can varry. A *MLP* can have one or more than more hidden layer, with one or more hidden units. The single hidden unit works same as *[simple Logist unit](https://image.ibb.co/b1mkJS/Single_Neuron.png)*.  The figure below is self descriptive of the concept of MLP:"},{"metadata":{"_uuid":"3684bc7862c1f643b7821f0d3386c3834d6e856a","_cell_guid":"c307c74f-d23b-4725-bbd4-b93ae9f57258"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/mlp/1.jpg?raw=true)"},{"metadata":{"_uuid":"5a7dc234f706cfb08f0273abe3a68712569a67cc","_cell_guid":"8e41d80f-fb56-416c-96de-74461c036945"},"cell_type":"markdown","source":"The above architecutre is unfolded to *Three layer Neural Network* in the figure below:"},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/mlp/2.jpg?raw=true)"},{"metadata":{"_uuid":"57918c497408757fde56a70ff2eba3d0d17abd3e","_cell_guid":"e6d0b0fe-dbdc-43f2-957d-5cea9665f6f5"},"cell_type":"markdown","source":"Let's consider the single feature of single training example, that is Feeded Forward thorugh the network. By Feed Forward we menan that the data is passed through left to right of network. the data is mulitplied by some weight *$w_1$* and bias *$b_1$* is added to it and then a non-linearity function *tanh* is applied to *$z_1$* values, resulting in *$a_1$*.  This *$a_1$* is then computed by next state *$a_2$* non-linearity functon, by having multiplication of *$w_2$* and addition of *$b_2$*. The same proces then continues to the out layer, to compute loss/cost of network.\nThe above process can be generalized, by passing input of shape **($n_x, m$)** and parameters **$w$** and **$b$** converted to shape of pervious and next hidden units/ activations. The subscripts in the each state above shows the hidden layer number.\nNow, let's pass some input data of shape **(5,2)** with network structure of **two hidden layers**. In first layer we have used **four hidden units** and **three hidden units** in the second layer. The data flow and its shape consistency during each state is shown below:\n"},{"metadata":{"_uuid":"9adacb534c1a475c6d68e745f719f54a774c4e7d","_cell_guid":"bd42e1ba-c67e-44ed-b37d-55f49b34d8e4"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/mlp/3.jpg?raw=true)"},{"metadata":{"_uuid":"705edcfa7cb12716234dcb847387da292e4f1c78","_cell_guid":"d0adcf23-0014-442e-ae7f-0e84608e9964"},"cell_type":"markdown","source":"In the following below, we will generalize this concept, and write down a pseudo code. and then implement it to actual code to get cost of network.\n\n                    For Loop l (1 to L): # L is the total number of hidden layer\n $$w_l = \\textrm{(next # of hidden units, previous # of input feature/ # of hidden units)}$$                                              \n$$b_l = \\textrm{(next # of hidden units, 1)}$$                                              \n\n$$z_l = w_l * prev_a + b_l $$                                              \n\n$$a_l = \\textrm{Compute activations each term}(z_l)$$                                              \n\n                  cost = logistic difference of  a_L , y"},{"metadata":{"_uuid":"23a59ce4864c10a3c7ac8d67763a11ad662e94ea","_cell_guid":"16459ccb-e31f-49a0-833f-abbd07f6bbf4"},"cell_type":"markdown","source":"**initialize_param**: This helper function will create parameter w's and b's for each of neural network layer; the inital value of w's will be random and b's will be zero. The size of w's and b's will be as follow: Since, in the first hidden layer, the parameter w's are multiplied by x's i.e. each of input feature of x is multiplied by respective weight w and adding a bias b is resulted into single value z that is passed to non-linear function resulting a single unit (neuron). And if we have n number of neurons then the size of weight parameter will be of shape (number of neurons, number of input feature) and the bias b will be of shape (number of neurons, 1). When focus on the second layer of network, then the generic idea of parameter will be same but instead the shape changes according to layers presented there. So, we have paramter w of shape (number of output values, number of neurons) and bias b will be of shape (number of output values, 1)."},{"metadata":{"collapsed":true,"_uuid":"8e9ac72d1d4955d1137766b4565b5fac52f0f956","_cell_guid":"df418712-d8af-4f68-8395-6271a721c9de","trusted":false},"cell_type":"code","source":"def initialize_paramaters(layer_dims, pt = False):\n    # return weights and bias of required network shape\n        #layer_dims: layer dim\n        # pt: wehter to print shapes\n    parameters = {}\n    L = len(layer_dims)\n    for l in range(1,L):\n        parameters['w'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n        parameters['b'+str(l)] = np.zeros(shape=(layer_dims[l], 1))\n        \n        assert(parameters['w' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n        \n        if(pt == True):\n            print('w'+str(l))\n            print(layer_dims[l],layer_dims[l-1])\n    return(parameters)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"517d758446fb541a4ef07996c851a7839b43b949","_cell_guid":"001a0d09-f7c2-423f-8244-0413a260ac47"},"cell_type":"markdown","source":"**Non-Linear Function:** Create two non-linear function *sigmoid* and *relu*."},{"metadata":{"collapsed":true,"_uuid":"2516a4f5877ab353e1889d7f5341f37b9cbfa1d4","_cell_guid":"bf7bec75-02fa-4e77-91d3-d6550d5be644","trusted":false},"cell_type":"code","source":"def sigmoid(z):\n    # Takes input as z and return sogmoid of value\n    s = 1 / (1 + np.exp(-z))\n    return s","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"268d80ff5ab4f21625a11c375e38364ccfaeb66e","_cell_guid":"9b69c6d4-f83f-49e8-9489-c3a08a7a21cd","trusted":false},"cell_type":"code","source":"def relu(z):\n    # Takes input as z and return relu of value    \n    r = np.maximum(0, z)\n    return r","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b96fe6c07dd5e152138d048894021aea3a1fc68","_cell_guid":"0829546a-5607-470a-b87c-d1c2087ee0e3"},"cell_type":"markdown","source":"**single_layer_forward_pass:** The helper function works as a *[simple Logist unit](https://image.ibb.co/b1mkJS/Single_Neuron.png)* like it compute linear and non-linear values. There are two  non-linear functions here, (i). **$sigmoid$**, (ii). **$relu$**"},{"metadata":{"collapsed":true,"_uuid":"861dd1c182b387171295af8b330f1b69f6db2e23","_cell_guid":"95266bfc-eeea-4166-94ef-1311ddb97fbd","trusted":false},"cell_type":"code","source":"def single_layer_forward_pass(prev_a, w, b, act_fun):\n    # return activations a and z\n        # prev_a: last layer activations values\n        # w, b : parameters weight and bias\n        # act_func: either sigmoid or relu\n    z = np.dot(w,prev_a) + b\n    a = np.zeros((z.shape))\n    if(act_fun == 'sigmoid'):\n        a = sigmoid(z)\n    elif(act_fun == 'relu'):\n        a = relu(z)\n    return(a,z)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ad6d7ecfce61144461d16722467a8c110d0a567","_cell_guid":"8fffc2bb-5a5b-4bcb-ab2a-69a41734dffc"},"cell_type":"markdown","source":"**multi_layer_forward_pass: ** This function compute actiovation at the output layer. It simply call above functions and steps over the multiple layers to output layer, by looping over each of the single layer."},{"metadata":{"collapsed":true,"_uuid":"4ead8465fccdd755a46d37ddfe58cc423448882b","_cell_guid":"b1c59617-d5e2-4038-a630-f45bac667ca4","trusted":false},"cell_type":"code","source":"def multi_layer_forward_pass(x, parameters, layer_dims, act_fun, pt = False):\n    # return activations at output layer\n        # x: input data\n        # parameters: dictonary object of weights and bias\n        # layer_dims: layer dim\n        # act_fun: activation function either relu or sigmoid\n        # pt: wehter to print shapes\n    L = len(layer_dims)\n    prev_a = x\n    activations = {}\n    for l in range(1,L):\n        if(l == L-1):\n#             print('last layer')\n            w = parameters['w'+str(l)]\n            b = parameters['b'+str(l)]\n            a, z = single_layer_forward_pass(prev_a, w, b, 'sigmoid')\n            activations['a'+str(l)] = a\n            \n            assert((a.shape) == (w.shape[0], prev_a.shape[1]))\n            \n            if(pt == True):\n                print('w'+str(l))\n                print(a.shape)\n        else:\n            w = parameters['w'+str(l)]\n            b = parameters['b'+str(l)]\n            a, z = single_layer_forward_pass(prev_a, w, b, act_fun)\n            prev_a = a\n            if(pt == True):\n                print('w'+str(l))\n                print(a.shape)\n            activations['a'+str(l)] = a\n            \n            assert((a.shape) == (w.shape[0], prev_a.shape[1]))\n            \n    return activations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6d63166e4e6f375c238625c8a69a785f60a6578","_cell_guid":"da7decf6-d7db-4035-8c9f-c43535225732"},"cell_type":"markdown","source":"**compute_cost:** The compute cost function, calculate loss of each of the training example in the data set by comparing difference between actual and produce activation from the network and then average all the loss to get a single cost value."},{"metadata":{"collapsed":true,"_uuid":"d54804e53b924fd6681cc116166b923a21603177","_cell_guid":"12e850d4-25f8-4c02-adab-4a6c05834948","trusted":false},"cell_type":"code","source":"def compute_cost(y, activations, layer_dims, m):\n    # calculate cost w.r.t. activation activations and actual value y\n        # y: target value\n        # activations: dictionary of activations\n        # layer_dims: network layers dimensions\n        # m: number of training examples in data set\n    L = len(layer_dims)   \n    a = activations['a'+ str(L-1)]\n    loss = -1*(y* np.log(a) + (1-y) * np.log(1-a))\n    cost = np.sum(loss)/m\n    return(cost)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbf7d7f8a53069499cddbc87c89ffe8f3ab30899","_cell_guid":"736538f9-7cff-4197-9021-b1c1cf20bd0a"},"cell_type":"markdown","source":"### Backward Propagation:\nIn the above section, we have implement neural network forward pass and computed cost of the network, now we will implement the Back-Propagation algorithm to update parameters *w's* and *b's* for optimizing network.\nThe below diagram shows the typical flow of backward computation. As in Forward propagtion, we moved from left to rigth, in this process we will go from right to left update parameters.\n"},{"metadata":{"_uuid":"5b1f87aa0e04c8d198223decefb295addc832eb5","_cell_guid":"326c1b07-9ba2-4c47-a746-ada3dd6c1928"},"cell_type":"markdown","source":"![](https://github.com/hamzafar/deep_learning_toys/blob/master/images/mlp/4.jpg?raw=true)"},{"metadata":{"_uuid":"f8964b1e78eed6944d5620ae60460f44836c8e32","_cell_guid":"5de7aee3-2fb3-45dd-b618-02e1296bc4bf"},"cell_type":"markdown","source":"In the figure above, we only considered three layers architecture. Using this architecture we will first derive equations for partial derivatives of parameters *w's* and *b's* then we will generalize equation to implement it deeper network. Beow are the equations for partial derivatives of above nodes:"},{"metadata":{"_uuid":"8ef264002fe84c3691d2433a72d181e8e7936d03","_cell_guid":"f99e61f0-36dc-483f-8c54-121edb6fb0fb"},"cell_type":"markdown","source":"\\begin{equation}\n\\partial{z_3} = a_3-y_3\\\\\n\\partial{b_3} = \\partial{z_3} \\tag{1}\\\\\n\\partial{w_3} = a_2 * \\partial{z_3}\\\\\n\\partial{z_2} = (w_3 *\\partial{z_3}) * \\partial{a_2}\\\\\n\\textrm{ In case of tanh: }  \\partial{a} = (1-a^2)\\\\\n\\textrm{ In case of sigmoid: }  \\partial{a} = (1-a)*a\\\\\n\\begin{split}\n\\textrm{ In case of relu: }  \\partial{a}&=  0 \\textrm{ if } a<=0\\\\\n&=1 \\textrm{ if } a>0\\\\\n\\end{split}\\\\\n\\partial{b_2} = \\partial{z_2}\\\\\n\\partial{w_2} = a_1 * \\partial{z_2}\\\\\n\\partial{z_1} = (w_2 *\\partial{z_2}) * \\partial{a_1}\\\\\n\\partial{b_1} = \\partial{z_1}\\\\\n\\partial{w_2} = x * \\partial{z_1}\n\\end{equation}\n\n"},{"metadata":{"_uuid":"fdca49709b046fb8314c698640fd4cbeefb2eb37","_cell_guid":"5766b0de-7a32-47cb-95f1-a15b02451832"},"cell_type":"markdown","source":"From the above, we have found a pattern i.e. the $\\partial{b}$ is equal to $\\partial{z}$ of same hidden layer, and $\\partial{w}$ is product of $\\partial{z}$ of same layer and activations or input values of previous layer. \n\nThe  exceptation, we have made in last layer, as we just skipped in between steps and computed $\\partial{z}$ as we know thre will only sigmoid function at output layer"},{"metadata":{"_uuid":"429d697f0f6de6d1b3daa3f80aa5bb535d4dcb57","_cell_guid":"6ec356b9-a8a4-4342-8803-dec084cd3588"},"cell_type":"markdown","source":"                                        For Loop(L to 1):\n                        \n                                            if(L = n-l): # last layer of network\n$$\\partial{z_L} = a_L - y$$\n\n                                            else:\n                    \n$$\\partial{z_L} = (w_{L+1} * \\partial{z_{L+1}}) * da$$\n$$\\textrm{where: } da \\textrm{ is partial derivative of tanh, relu, and sigmoid } $$\n$$\\partial{b_L} = \\partial{d_{L}}$$\n$$\\partial{w_L} = a_{L-1}*\\partial{d_{L}}$$\n\n"},{"metadata":{"_uuid":"513f0d257e3dacba1fc02e444262519830808a4d","_cell_guid":"9baa00f7-ef82-49a7-899d-2b942a38d580"},"cell_type":"markdown","source":"**derivatives of non-linear functions:** As, we have dervied, partial darivatives of non-linear functions, the below two pyhthon functions *sigmoid_deri and relu_deri* will implement the equations."},{"metadata":{"collapsed":true,"_uuid":"793a32be21f3f73e008fbbe9f05947b48102ed75","_cell_guid":"ec970f0c-2fdd-4ad8-a23e-79a570b5ba02","trusted":false},"cell_type":"code","source":"def sigmoid_deri(a):\n    # return partial derivative of sigmoid\n        #a: activation\n    da = (1-a) * a\n    return da","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c0b2a20c7f3e5457d9d296e62232c835f223a361","_cell_guid":"d54a4cd3-30a7-460e-bde9-4387ed4d2c23","trusted":false},"cell_type":"code","source":"def relu_deri(a):\n    # return partial derivative of relu\n        #a: activation\n    da = np.array(a, copy=True)\n    da[a <= 0] = 0\n    return da","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90ee97db5d36843e66dd862d8e802e45cac68b83","_cell_guid":"5edd849a-d4ec-4b49-8ace-00f4e4c231f5"},"cell_type":"markdown","source":"**derivaive_parameters: **  According to generic equations, we have derived in pesudo code above, *derivaive_parameters* will impement the equations for taking paritial derivative of weight *w's* and bias *b's*. The equation is as follows:\n$$\\partial{w_L} = a_{L-1}*\\partial{d_{L}}$$\n$$\\partial{b_L} = \\partial{d_{L}}$$\n"},{"metadata":{"collapsed":true,"_uuid":"1a06931de5836e6795af253ef5bf138021368396","_cell_guid":"c6496ee6-4271-45ba-be77-5743425db1ce","trusted":false},"cell_type":"code","source":"def derivaive_parameters(dz, a):\n    # compute partial derivatives of w and b\n        # dz: partial derivative of z\n        # a: activations\n    dw = (1 / m) * np.dot(dz, a.T)\n    db = (1 / m) * np.sum(dz, axis=1, keepdims=True)\n    return(dw, db)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1949a07c32bfb899cfa174e2f111298da452196","_cell_guid":"76bc6e94-4ec9-4e79-9cbb-d926b0c32f35"},"cell_type":"markdown","source":"**derivaive_z: ** This helper function computes the derivate of liner functions:\n$$\\partial{z_L} = (w_{L+1} * \\partial{z_{L+1}}) * da$$"},{"metadata":{"collapsed":true,"_uuid":"e321a8ecce86646c706b9e7ccfacd09919efa250","_cell_guid":"40ea41f8-66ef-4869-88f1-023287f9c23f","trusted":false},"cell_type":"code","source":"def derivaive_z(dz, w, da):\n    # computes derivative of z\n        # dz: next node dz\n        # w: paramter weight of next node\n        # da: partial derivative of activation functions.\n    dz = np.dot(w.T, dz) * da\n    return dz","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63775e703c897e551c5ebdb167129b125dbf8eda","_cell_guid":"a01e175f-7ace-4782-96b6-af9fa6dc6d3f"},"cell_type":"markdown","source":"**back_propagate: ** This functions implements the pesudo code algorithm, we have mentioned in the backpropagation section beneath derivative w.r.t cost equation."},{"metadata":{"collapsed":true,"_uuid":"df70ec36f1937a4466d358368b1b8fc1938d6c4e","_cell_guid":"ff04bd94-a0ff-4d91-b398-55e5265d9ca0","trusted":false},"cell_type":"code","source":"def back_propagate(x, y, m, parameters, activations, layer_dims, act_fun):\n    # return gradeint of paramters\n        # x: input data\n        # y: target label\n        # m: number of training examples in data set\n        # parameters: dictonary object of weights and bias\n        # activations: dictionary object containing activation on different layers\n        # layer_dims: layer dim\n        # act_fun: activation function either relu or sigmoid\n\n    L = len(layer_dims)\n    grads = {}\n    grads_line = {}\n\n    for l in reversed(range(L-1)):\n        l = l+1\n        if(l == L-1):\n#             print('a'+str(l))\n    #         print(activations['a'+str(l+1)])\n            grads_line['dz'+str(l)] = activations['a'+str(l)] - y\n        \n            grads['dw'+str(l)], grads['db'+str(l)] = derivaive_parameters(grads_line['dz'+str(l)], activations['a'+str(l-1)])\n            \n            assert(grads['dw'+str(l)].shape) == (parameters['w'+str(l)].shape)\n            assert(grads['db'+str(l)].shape) == (parameters['b'+str(l)].shape)\n        else:\n#             print('a'+str(l))\n            da = np.empty(activations['a'+str(l)].shape)\n\n            if(act_fun == 'sigmoid'):\n                da = sigmoid_deri(activations['a'+str(l)])            \n            elif(act_fun == 'relu'):\n                da = relu_deri(activations['a'+str(l)])\n\n            grads_line['dz'+str(l)] = derivaive_z(grads_line['dz'+str(l+1)], parameters['w'+str(l+1)], da)\n        \n            if(l-1 != 0):\n                grads['dw'+str(l)], grads['db'+str(l)] = derivaive_parameters(grads_line['dz'+str(l)], activations['a'+str(l-1)])\n            else:\n                 grads['dw'+str(l)], grads['db'+str(l)]  = derivaive_parameters(grads_line['dz'+str(l)], x)\n                    \n            assert(grads['dw'+str(l)].shape) == (parameters['w'+str(l)].shape)\n            assert(grads['db'+str(l)].shape) == (parameters['b'+str(l)].shape)\n                \n    return grads","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dc04cba37016f4051d5a470fb681f18b655c4e3","_cell_guid":"738b32d7-123b-41ba-be94-7512715fb116"},"cell_type":"markdown","source":"**update_parameters:** This function update paramaters w's and b's according to the following equation:\n$$w := w - \\partial{w} * \\textrm{learning rate}$$\n$$b := b - \\partial{b} * \\textrm{learning rate}$$"},{"metadata":{"collapsed":true,"_uuid":"d110eba40e53899de48cf30d814de0f6f0e0bfd9","_cell_guid":"8d7a017f-0a55-4447-9e34-03d0ae42fe0d","trusted":false},"cell_type":"code","source":"def update_parameters(parameters, grads, layer_dims):\n    # update and return parameters \n        # parameters: dictonary object of w's and b's\n        # grads: dictionary: object of gradient of w's and b's\n        # layer_dims: layer dim\n\n    L = len(layer_dims)\n    for l in range(1,L):\n#         print(l)\n        parameters['w'+str(l)] = parameters['w'+str(l)] -(lr * grads['dw'+str(l)])\n        parameters['b'+str(l)] = parameters['b'+str(l)] -(lr * grads['db'+str(l)])\n    return parameters    ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"60d87b37172dd8be08c592e3a61dd12014133b85","_cell_guid":"e29fb612-9722-4d61-b7e4-e267d943163f","trusted":false},"cell_type":"code","source":"def optimize_parameters(x, y, parameters, act_fun, layer_dims, m, num_iter):\n    lst_cost = []\n    \n    for i in range(num_iter):\n        activations = multi_layer_forward_pass(x, parameters, layer_dims, act_fun, pt = False)\n        cost = (compute_cost(y, activations, layer_dims, m))\n        grads = back_propagate(x, y, m, parameters, activations, layer_dims, act_fun)\n        parameters = update_parameters(parameters, grads, layer_dims)\n        lst_cost.append(cost)\n#         lst_cost.append(format(cost, '.4f'))\n    \n    return (lst_cost, parameters)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"28a5472a63998ffa688a591fe398b7bde0af9aca","_cell_guid":"80c54ccf-1f0c-4231-93c0-ff792797e4d2","trusted":false},"cell_type":"code","source":"def plt_res(lst, ylab, lr):\n    #This will plot the list of values at y axis while x axis will contain number of iteration\n    #lst: lst of action/cost\n    #ylab: y-axis label\n    #lr: learning rate\n    plt.plot(lst)\n    plt.ylabel(ylab)\n    plt.xlabel('iterations')\n    plt.title(\"Learning rate =\" + str(lr))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8515dc980a3fee0cd5fc5b70d137890ea8481aef","_cell_guid":"130a2225-e7ca-45e7-80f1-2796b56e939a"},"cell_type":"markdown","source":"we have implemented Regression using 10000 , 100000 and 1000000 samples and sotred their respective learned paramteres (weights and bias) and cost of the function."},{"metadata":{"collapsed":true,"_uuid":"3df286e106c30221f5f6e5f2f3601a6acd58131d","_cell_guid":"f17e5317-8774-4528-b2b1-287ac5332a35","trusted":false},"cell_type":"code","source":"n_x = 50\nn_y = 1\nm = 10000\nlr = 0.5\nnum_iter = 30\n\nx = generate_bits(n_x, m)\ny = generate_label(x, m)\n\nlayer_dims = [n_x,4,3,n_y]\nact_fun = 'relu'\n\nparameters = initialize_paramaters(layer_dims, pt = False)\n\nlst_cost_s, parametes_s = optimize_parameters(x, y, parameters, act_fun, layer_dims, m, num_iter)\n\n# plt_res(lst_cost, 'cost', lr)\n\nm = 100000\nx = generate_bits(n_x, m)\ny = generate_label(x, m)\n\nparameters = initialize_paramaters(layer_dims, pt = False)\nlst_cost_m, parametes_m = optimize_parameters(x, y, parameters, act_fun, layer_dims, m, num_iter)\n\n\nm = 1000000\nx = generate_bits(n_x, m)\ny = generate_label(x, m)\n\nparameters = initialize_paramaters(layer_dims, pt = False)\nlst_cost_l, parametes_l = optimize_parameters(x, y, parameters, act_fun, layer_dims, m, num_iter)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"365ea55e83b322ddbbb85dc1c7c22271359018f1","_cell_guid":"1c07b1bc-d53e-4a6c-96b2-2f7d8afba2a7"},"cell_type":"markdown","source":"**Prediction** In prediciton step, we have done following two steps:\n\n1. Calculate $ Y$^ $=A=Ïƒ(w^T * X+b) $\n1. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction\nTo validate how the MLP is performing; we just created new dataset of 0.1 times m size and computed its predictions from the get_prediction function. The label of created data is also generated that are matched with the prediction values to get accuracy. Since we have trained weights for two different datasets, we computed the accuracy for both."},{"metadata":{"collapsed":true,"_uuid":"d4f6b201a950da876d60384ce84e549d0399e575","_cell_guid":"261d3ddb-c15c-45ad-bbb6-d2828f4aeb22","trusted":false},"cell_type":"code","source":"def get_prediction(x, parameters, layer_dims, act_fun,  m):\n    # returns the prediction on the dataset\n        # x: input data (unseen)\n        # w, b: parameters weights and bias\n        # m: total sample set\n    L = len(layer_dims)   \n    \n    activations = multi_layer_forward_pass(x, parameters, layer_dims, act_fun, pt = False)\n    a = activations['a'+ str(L-1)]\n    y_prediction = np.zeros((1, m))\n    for i in range(a.shape[1]):\n        y_prediction[0,i] = 1 if a[0, i] > 0.5 else 0\n    return(y_prediction)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b2009310b64d324055ffe70c8d282bb185baafb4","_cell_guid":"b01f8d5d-4675-4014-b8d8-ad0a44f724a7","trusted":false},"cell_type":"code","source":"def get_accuracy(y, y_prediction, m):\n    # return the accuracy by calculated the difference between actual and predicted label\n        # y: actual values\n        # y_prediction: prediction acquired from the get_prediction\n        # m: total number of sample\n    df = pd.DataFrame()\n    df['actual'] = y[0]\n    df['prediction'] = y_prediction[0]\n    df['compare']= df['prediction'] == df['actual']\n\n#     print(df[df['compare']==True])\n#     print('Accuracy: ' ,len(df[df['compare']==True]['compare'])/m)\n    return(len(df[df['compare']==True]['compare'])/m)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1866820a83c73c08870a661bc66a0128cdb37fc7","_cell_guid":"5294d11d-e6af-4961-89a7-f93176708e54","trusted":false},"cell_type":"code","source":"tm = int(0.1 * m)\nx = generate_bits(n_x, tm)\ny = generate_label(x, tm)\n\ny_prediction = get_prediction(x, parametes_s, layer_dims, act_fun,  tm)\nacc_s = get_accuracy(y, y_prediction, tm)\n\ny_prediction = get_prediction(x, parametes_m, layer_dims, act_fun,  tm)\nacc_m = get_accuracy(y, y_prediction, tm)\n\ny_prediction = get_prediction(x, parametes_l, layer_dims, act_fun,  tm)\nacc_l = get_accuracy(y, y_prediction, tm)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bdd4d1aee910d04a8690f78a8bf8d4e7823e6f6e","_cell_guid":"bfe19248-f073-4813-9118-031362852087","trusted":false},"cell_type":"code","source":"print('------- 10000 training set-------------')\nprint('Accurcy at 10000 training set: ', acc_s)\nplt_res(lst_cost_s, 'cost', lr)\n\nprint('-------100000 training set-------------')\nprint('Accurcy at 100000 training set: ', acc_m)\nplt_res(lst_cost_m, 'cost', lr)\n\nprint('-------1000000 training set-------------')\nprint('Accurcy at 1000000 training set: ', acc_l)\nplt_res(lst_cost_l, 'cost', lr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2190a85e0fc48daf4f1e095afa31f2495af42109","_cell_guid":"59cacd9f-be37-4acc-ac9f-d1a47f834a34"},"cell_type":"markdown","source":"## Discussion: ##\nWe have deployed Multi Layer Perceptrons with layer size $[n_x,4,3,n_y]$ and activation function $relu$. The parameters are optimized about 30 number of iteration. This same setting is used to learn weights on three different data-set size i.e. *10000, 100000 and 1000000*. Surprizingly, the network gives almost same accuracy ~50% of all three data sets. \n\nAlthough, the model coverengece to maximum, around 10 iteration and after that it shows a slow convergence to less cost."},{"metadata":{"_uuid":"76452425efac9324767a91811e9730c5fc343a72","_cell_guid":"7a01b48d-f8a8-4e29-aec2-aef329c293d1"},"cell_type":"markdown","source":"---"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}